# ğŸ§  Mimic Human-Level Understanding of Images  
### Graph-based Image Captioning with Dual Attention

![Python](https://img.shields.io/badge/Python-3.8+-blue)
![PyTorch](https://img.shields.io/badge/PyTorch-Deep%20Learning-red)
![Computer Vision](https://img.shields.io/badge/Domain-Computer%20Vision-green)
![NLP](https://img.shields.io/badge/Domain-Natural%20Language%20Processing-orange)
![GCN](https://img.shields.io/badge/Model-Graph%20Neural%20Network-purple)
![Attention](https://img.shields.io/badge/Model-Dual%20Multi--Head%20Attention-yellow)
![Status](https://img.shields.io/badge/Status-Research%20Prototype-informational)

---

## ğŸ“Œ Project Overview
This project implements a **researchâ€‘grade image captioning system** that generates **humanâ€‘like descriptions** by learning not only *what* objects appear in an image, but also *how they relate to each other*.

Unlike conventional CNNâ€‘RNN captioning pipelines, this system explicitly models **object relationships** using **Graph Convolutional Networks (GCNs)** and uses **Dual Multiâ€‘Head Attention** to align visual relations with natural language.

This work is based on the MSc project **â€œMimic Humanâ€‘Level Understanding of Imageâ€** (St. Xavierâ€™s College, Kolkata, 2024).

---

## ğŸ§  Core Idea

Traditional captioning models treat an image as a flat vector:

```
Image â†’ CNN â†’ RNN â†’ Caption
```

This project treats an image as a **structured graph** of interacting objects:

```
Image â†’ Objects â†’ Relation Graph â†’ GCN â†’ Dual Attention â†’ LSTM â†’ Caption
```

This enables reasoning like:

> â€œA man in a red shirt riding a motorcycleâ€  
instead of  
> â€œman motorcycleâ€

---

# ğŸ—ï¸ System Architecture

```
Input Image
     â”‚
     â–¼
VGG19 CNN (global + object features)
     â”‚
     â–¼
Fasterâ€‘RCNN Object Detection
     â”‚
     â–¼
Objectâ€‘Relationship Graph (IoUâ€‘based)
     â”‚
     â–¼
Graph Convolution Network (2 layers)
     â”‚
     â–¼
Edge Readout (2048â€‘D)
     â”‚
     â”œâ”€ Global CNN features (2048â€‘D)
     â–¼
Concatenation â†’ 4096â€‘D Encoder Embedding
     â”‚
     â–¼
Dual Attention (Self + Cross)
     â”‚
     â–¼
LSTM Decoder
     â”‚
     â–¼
Caption
```

---

# ğŸ” Image Encoding

## 1ï¸âƒ£ Global and Object Features
Two pretrained networks are used:

| Model | Purpose |
|------|--------|
| **VGG19** | Extracts spatial feature maps |
| **Fasterâ€‘RCNN (ResNetâ€‘50â€‘FPN)** | Detects objects and bounding boxes |

VGG19 outputs:
```
(batch, 49, 2048)
```

These 49 spatial regions represent different parts of the image.

---

## 2ï¸âƒ£ Objectâ€‘Relationship Graph

Each detected object becomes a **graph node**.  
Edges are created when two objects overlap beyond an **IoU threshold**.

If no objects are detected, the entire image becomes a **single node with a selfâ€‘loop**.

Each node stores **VGG19 features extracted from the cropped object region**.

---

## 3ï¸âƒ£ Graph Convolution Network (GCN)

Two graph convolution layers propagate relational information between objects:

```
H1 = ReLU(GCN1(G, H))
H2 = ReLU(GCN2(G, H1))
```

For each edge (u â†’ v):

```
EdgeFeature = concat(H2[u], H2[v])
```

All edge features are passed through a linear layer and **meanâ€‘pooled** to produce a **2048â€‘D graph embedding**.

This is concatenated with **2048â€‘D CNN features â†’ 4096â€‘D encoder output**.

---

# ğŸ§  Caption Decoder

The decoder uses **Dual Multiâ€‘Head Attention**:

### ğŸ”¹ Multiâ€‘Head Selfâ€‘Attention
Models wordâ€‘toâ€‘word dependencies inside the generated caption.

### ğŸ”¹ Multiâ€‘Head Crossâ€‘Attention
Aligns each generated word with **imageâ€‘graph features**.

### ğŸ”¹ LSTM Generator
Combines both attention contexts to predict the next word.

```
(Selfâ€‘Attention + Crossâ€‘Attention) â†’ LSTM â†’ Softmax â†’ Next Word
```

Attention maps can be visualized to show **which image regions influence each word**.

---

# ğŸ§ª Training Pipeline

## Dataset
**Flickr8k**
- 8,000 images
- 5 captions per image

(Designed to scale to Flickr30k / MSâ€‘COCO.)

---

## Preprocessing

**Images**
```
Resize â†’ 224Ã—224 â†’ Normalize
```

**Captions**
- Tokenized using spaCy
- Lowercased
- Vocabulary built with frequency threshold
- <SOS> and <EOS> tokens added

---

## Graph Construction
For each image:
1. Fasterâ€‘RCNN detects objects
2. Crop object regions
3. Extract VGG19 features
4. Build IoUâ€‘based graph

---

## Training

At timestep t:

```
Previous words â†’ Self Attention
Image relations â†’ Cross Attention
â†’ LSTM â†’ Predict next word
```

Loss:  
```
Crossâ€‘Entropy
```

Optimizer:  
```
Adam
```

---

# ğŸ“Š Results

| Metric | Score |
|-------|-------|
| BLEUâ€‘1 | **â‰ˆ 0.55** |
| BLEUâ€‘2 | **â‰ˆ 0.33** |

The GCN + Dual Attention model significantly outperforms the Biâ€‘LSTM baseline and produces captions that better capture **object interactions, colors, and actions**.

---

# ğŸ› ï¸ Tech Stack
- PyTorch
- VGG19
- Fasterâ€‘RCNN
- DGL (Graph Learning)
- Multiâ€‘Head Attention
- LSTM
- spaCy

---

# ğŸ”® Future Work
- Larger datasets (MSâ€‘COCO, Flickr30k)
- Sceneâ€‘graph supervision
- Transformerâ€‘based decoders
- Customâ€‘trained object detectors

---

# ğŸ“„ Full Project Documentation

The complete technical report (methodology, algorithms, experiments, and results) is available here:

ğŸ‘‰ **Project Report (PDF):**  
[Download / View Full Documentation](https://drive.google.com/file/d/1U95B8e3opCeefdzLl1cd1G8cSxwVSRpG/view?usp=drive_link)

This document contains:
- Mathematical formulation of the GCN + Dual Attention model  
- Graph construction algorithm  
- Training and inference procedures  
- BLEU score evaluation  
- Attention visualizations and qualitative results  
- Limitations and future scope  
