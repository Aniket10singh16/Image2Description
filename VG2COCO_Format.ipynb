{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import json\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "N_class = 3000  # keep the top 3000 classes\n",
    "raw_data_dir = 'D:\\\\WORK\\\\M.SC\\\\MSC_Project\\\\GitHub\\\\Dataset\\\\VG\\\\'\n",
    "output_dir = 'D:\\\\WORK\\\\M.SC\\\\MSC_Project\\\\GitHub\\\\Image2Description\\\\'\n",
    "\n",
    "# ---------------------------------------------------------------------------- #\n",
    "# Load raw VG annotations and collect top-frequent synsets\n",
    "# ---------------------------------------------------------------------------- #\n",
    "\n",
    "with open(raw_data_dir + 'image_data.json') as f:\n",
    "    raw_img_data = json.load(f)\n",
    "with open(raw_data_dir + 'objects.json') as f:\n",
    "    raw_obj_data = json.load(f)\n",
    "\n",
    "# collect top frequent synsets\n",
    "all_synsets = [\n",
    "    synset for img in raw_obj_data\n",
    "    for obj in img['objects'] for synset in obj['synsets']]\n",
    "synset_counter = Counter(all_synsets)\n",
    "top_synsets = [\n",
    "    synset for synset, _ in synset_counter.most_common(N_class)]\n",
    "\n",
    "# ---------------------------------------------------------------------------- #\n",
    "# build raw \"categories\"\n",
    "# ---------------------------------------------------------------------------- #\n",
    "\n",
    "categories = [\n",
    "    {'id': (n + 1), 'name': synset} for n, synset in enumerate(top_synsets)]\n",
    "synset2cid = {c['name']: c['id'] for c in categories}\n",
    "\n",
    "# ---------------------------------------------------------------------------- #\n",
    "# build \"image\"\n",
    "# ---------------------------------------------------------------------------- #\n",
    "\n",
    "images = [\n",
    "    {'id': img['image_id'],\n",
    "     'width': img['width'],\n",
    "     'height': img['height'],\n",
    "     'file_name': str(img['image_id'])+'.jpg',\n",
    "     'coco_id': img['coco_id']}\n",
    "    for img in raw_img_data]\n",
    "\n",
    "# ---------------------------------------------------------------------------- #\n",
    "# build \"annotations\"\n",
    "# ---------------------------------------------------------------------------- #\n",
    "\n",
    "annotations = []\n",
    "skip_count_1, skip_count_2, skip_count_3 = 0, 0, 0\n",
    "for img in raw_obj_data:\n",
    "    for obj in img['objects']:\n",
    "        synsets = obj['synsets']\n",
    "        if len(synsets) == 0:\n",
    "            skip_count_1 += 1\n",
    "        elif len(synsets) > 1:\n",
    "            skip_count_2 += 1\n",
    "        elif synsets[0] not in synset2cid:\n",
    "            skip_count_3 += 1\n",
    "        else:\n",
    "            cid = synset2cid[synsets[0]]\n",
    "            bbox = [obj['x'], obj['y'], (obj['x']+obj['w']), (obj['y']+obj['h'])]\n",
    "            area = obj['w'] * obj['h']\n",
    "            ann = {'id': obj['object_id'],\n",
    "                   'image_id': img['image_id'],\n",
    "                   'category_id': cid,\n",
    "                   'segmentation': [],\n",
    "                   'area': area,\n",
    "                   'bbox': bbox,\n",
    "                   'iscrowd': 0}\n",
    "            annotations.append(ann)\n",
    "\n",
    "# ---------------------------------------------------------------------------- #\n",
    "# Save to json file\n",
    "# ---------------------------------------------------------------------------- #\n",
    "categories = [{'id': (n + 1), 'name': synset.split('.')[0]} for n, synset in enumerate(top_synsets)]\n",
    "with open(output_dir + 'instances_vg3k_raw.json', 'w') as f:\n",
    "    json.dump(\n",
    "        {'images': images,\n",
    "         'annotations': annotations,\n",
    "         'categories': categories}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split completed.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "vg3k_raw_json_file = 'D:\\\\WORK\\\\M.SC\\\\MSC_Project\\\\GitHub\\\\Image2Description\\\\instances_vg3k_raw.json'\n",
    "output_dir = 'D:\\\\WORK\\\\M.SC\\\\MSC_Project\\\\GitHub\\\\Image2Description\\\\'\n",
    "\n",
    "# Load raw VG annotations and COCO annotations\n",
    "with open(vg3k_raw_json_file) as f:\n",
    "    dataset_vg3k = json.load(f)\n",
    "\n",
    "vg3k_cls_names = dataset_vg3k['categories']\n",
    "\n",
    "# Split dataset randomly into train, val, and test\n",
    "total_images = len(dataset_vg3k['images'])\n",
    "indices = list(range(total_images))\n",
    "random.shuffle(indices)\n",
    "\n",
    "# Define split percentages\n",
    "train_split = 0.80\n",
    "val_split = 0.1\n",
    "test_split = 0.1\n",
    "\n",
    "# Calculate number of images for each split\n",
    "num_train = int(train_split * total_images)\n",
    "num_val = int(val_split * total_images)\n",
    "num_test = total_images - num_train - num_val\n",
    "\n",
    "# Split images\n",
    "images_train = [dataset_vg3k['images'][i] for i in indices[:num_train]]\n",
    "images_val = [dataset_vg3k['images'][i] for i in indices[num_train:num_train + num_val]]\n",
    "images_test = [dataset_vg3k['images'][i] for i in indices[-num_test:]]\n",
    "\n",
    "# Create sets of image IDs for faster lookup\n",
    "imgids_train = {img['id'] for img in images_train}\n",
    "imgids_val = {img['id'] for img in images_val}\n",
    "imgids_test = {img['id'] for img in images_test}\n",
    "\n",
    "# Split annotations\n",
    "annotations_train = [ann for ann in dataset_vg3k['annotations'] if ann['image_id'] in imgids_train]\n",
    "annotations_val = [ann for ann in dataset_vg3k['annotations'] if ann['image_id'] in imgids_val]\n",
    "annotations_test = [ann for ann in dataset_vg3k['annotations'] if ann['image_id'] in imgids_test]\n",
    "\n",
    "# Save to JSON file\n",
    "dataset_vg3k_train = {\n",
    "    'images': images_train,\n",
    "    'annotations': annotations_train,\n",
    "    'categories': vg3k_cls_names\n",
    "}\n",
    "dataset_vg3k_val = {\n",
    "    'images': images_val,\n",
    "    'annotations': annotations_val,\n",
    "    'categories': vg3k_cls_names\n",
    "}\n",
    "dataset_vg3k_test = {\n",
    "    'images': images_test,\n",
    "    'annotations': annotations_test,\n",
    "    'categories': vg3k_cls_names\n",
    "}\n",
    "\n",
    "with open(output_dir + 'instances_vg3k_cocoaligned_train.json', 'w') as f:\n",
    "    json.dump(dataset_vg3k_train, f)\n",
    "with open(output_dir + 'instances_vg3k_cocoaligned_val.json', 'w') as f:\n",
    "    json.dump(dataset_vg3k_val, f)\n",
    "with open(output_dir + 'instances_vg3k_cocoaligned_test.json', 'w') as f:\n",
    "    json.dump(dataset_vg3k_test, f)\n",
    "\n",
    "print('Dataset split completed.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Final_Project_V2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
