{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EXTREME_RED\\AppData\\Local\\Temp\\ipykernel_6692\\4289264615.py:4: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import string\n",
    "import os\n",
    "from PIL import Image\n",
    "import glob\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000268201_693b08cb0e.jpg#0\tA child in a pink dress is climbing up a set of stairs in an entry way .\n",
      "1000268201_693b08cb0e.jpg#1\tA girl going into a wooden building .\n",
      "1000268201_693b08cb0e.jpg#2\tA little girl climbing into a wooden playhouse .\n",
      "1000268201_693b08cb0e.jpg#3\tA little girl climbing the s\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "filename = \"Flickr8k.token.txt\"\n",
    "# load descriptions\n",
    "doc = load_doc(filename)\n",
    "print(doc[:300])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 19551 \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def read_json_file(json_file_path):\n",
    "    descriptions = {}\n",
    "    try:\n",
    "        with open(json_file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "            for item in data:\n",
    "                if 'image_id' in item and 'paragraph' in item:\n",
    "                    image_id_str = str(item['image_id'])\n",
    "                    descriptions[image_id_str] = item['paragraph']\n",
    "                else:\n",
    "                    print(\"Invalid format: Missing 'image_id' or 'paragraph' key in JSON item.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"File not found.\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Invalid JSON format in file.\")\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e)\n",
    "    return descriptions\n",
    "\n",
    "\n",
    "json_file_path = 'paragraphs_v1.json'  # Specify the path to your JSON file\n",
    "descriptions = read_json_file(json_file_path)\n",
    "#print(\"Descriptions:\", descriptions)\n",
    "print('Loaded: %d ' % len(descriptions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2356347', '2317429', '2414610', '2365091', '2383120']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(descriptions.keys())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image ID: 2356347, Paragraph: A large building with bars on the windows in front of it. There is people walking in front of the building. There is a street in front of the building with many cars on it. \n",
      "Image ID: 2317429, Paragraph: A white round plate is on a table with a plastic tablecloth on it. Two foil covered food halves are on the white plate along with a serving of golden yellow french fries. Next to the white plate in a short, topless, plastic container is a white sauce. Diagonal to the white plate are the edges of several other stacked plates. There are black shadows reflected on the table.\n",
      "Image ID: 2414610, Paragraph: A woman in a blue tennis outfit stands on a green tennis court. She is swinging a blue tennis racket. There is a green tennis ball above her head. \n",
      "Image ID: 2365091, Paragraph: A large red and white train is traveling on tracks in a what looks to be a rural area. There are trees and hills in the background and the ground looks dry. The train has many large windows for the passengers to look out of. The train is mostly white with red on the front upper part of the train and red stripes and trim on the sides. The roof of the train is grey.\n",
      "Image ID: 2383120, Paragraph: A very clean and tidy a bathroom. Everything is a neat porcelain white. This bathroom is both retro and modern.\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for image_id, paragraph in descriptions.items():\n",
    "    print(f\"Image ID: {image_id}, Paragraph: {paragraph}\")\n",
    "    count += 1\n",
    "    if count == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A large building with bars on the windows in front of it. There is people walking in front of the building. There is a street in front of the building with many cars on it. \n"
     ]
    }
   ],
   "source": [
    "print(descriptions['2356347'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def clean_descriptions(descriptions):\n",
    "    # prepare translation table for removing punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for key, desc in descriptions.items():\n",
    "        # tokenize\n",
    "        desc = desc.split()\n",
    "        # convert to lower case\n",
    "        desc = [word.lower() for word in desc]\n",
    "        # remove punctuation from each token\n",
    "        desc = [w.translate(table) for w in desc]\n",
    "        # remove hanging 's' and 'a'\n",
    "        desc = [word for word in desc if len(word)>1]\n",
    "        # remove tokens with numbers in them\n",
    "        desc = [word for word in desc if word.isalpha()]\n",
    "        # store as string\n",
    "        descriptions[key] = ' '.join(desc)\n",
    "\n",
    "# clean descriptions\n",
    "clean_descriptions(descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'woman in blue tennis outfit stands on green tennis court she is swinging blue tennis racket there is green tennis ball above her head'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descriptions['2414610']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 13449\n"
     ]
    }
   ],
   "source": [
    "def create_vocabulary(descriptions):\n",
    "    vocabulary = set()\n",
    "    for key, desc in descriptions.items():\n",
    "        # Tokenize the paragraph into words\n",
    "        words = desc.split()\n",
    "        # Add unique words to the vocabulary set\n",
    "        vocabulary.update(words)\n",
    "    return vocabulary\n",
    "\n",
    "# Create vocabulary\n",
    "vocabulary = create_vocabulary(descriptions)\n",
    "#print(\"Vocabulary:\", vocabulary)\n",
    "print(\"Vocabulary Size:\", len(vocabulary))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_descriptions_to_file(descriptions, file_path):\n",
    "    with open(file_path, 'w') as file:\n",
    "        for key, desc in descriptions.items():\n",
    "            file.write(f\"{key} {desc}\\n\")\n",
    "\n",
    "\n",
    "file_path = 'descriptions.txt'  \n",
    "save_descriptions_to_file(descriptions, file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_json_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(\"File not found.\")\n",
    "        return {}\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Invalid JSON format in file.\")\n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e)\n",
    "        return {}\n",
    "\n",
    "def save_descriptions_to_file(descriptions, file_path):\n",
    "    with open(file_path, 'w') as file:\n",
    "        for key, desc in descriptions.items():\n",
    "            file.write(f\"{key} {desc}\\n\")\n",
    "\n",
    "# Load train_images.json file\n",
    "test_images_file_path = 'train_split.json'\n",
    "with open(test_images_file_path, 'r') as file:\n",
    "    test_images_data = json.load(file)\n",
    "\n",
    "# Convert image IDs to strings\n",
    "image_ids = [str(image_id) for image_id in test_images_data]\n",
    "\n",
    "# Filter descriptions dictionary to include only image IDs present in train_images.json\n",
    "filtered_descriptions = {key: desc for key, desc in descriptions.items() if key in image_ids}\n",
    "\n",
    "# Save filtered descriptions to a text file\n",
    "file_path = 'train_descriptions.txt'\n",
    "save_descriptions_to_file(filtered_descriptions, file_path)\n",
    "#print(\"Filtered descriptions saved to\", file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2489\n"
     ]
    }
   ],
   "source": [
    "# Load test_images.json file\n",
    "test_images_file_path = 'test_split.json'\n",
    "with open(test_images_file_path, 'r') as file:\n",
    "    test_images_data = json.load(file)\n",
    "\n",
    "# Convert image IDs to strings\n",
    "image_ids = [str(image_id) for image_id in test_images_data]\n",
    "\n",
    "# Filter descriptions dictionary to include only image IDs present in test_images.json\n",
    "filtered_descriptions = {key: desc for key, desc in descriptions.items() if key in image_ids}\n",
    "print(len(filtered_descriptions))\n",
    "# Save filtered descriptions to a text file\n",
    "file_path = 'test_descriptions.txt'\n",
    "save_descriptions_to_file(filtered_descriptions, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2487\n"
     ]
    }
   ],
   "source": [
    "# Load val_images.json file\n",
    "test_images_file_path = 'val_split.json'\n",
    "with open(test_images_file_path, 'r') as file:\n",
    "    test_images_data = json.load(file)\n",
    "\n",
    "# Convert image IDs to strings\n",
    "image_ids = [str(image_id) for image_id in test_images_data]\n",
    "\n",
    "# Filter descriptions dictionary to include only image IDs present in val_images.json\n",
    "filtered_descriptions = {key: desc for key, desc in descriptions.items() if key in image_ids}\n",
    "print(len(filtered_descriptions))\n",
    "# Save filtered descriptions to a text file\n",
    "file_path = 'val_descriptions.txt'\n",
    "save_descriptions_to_file(filtered_descriptions, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed descriptions saved to: preprocessed_filtered_train_descriptions.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torchtext\n",
    "from torchtext.vocab import GloVe\n",
    "\n",
    "# Function to preprocess a single description\n",
    "def preprocess_description(description, tokenizer, stopwords):\n",
    "    tokenized = tokenizer(description.lower())\n",
    "    tokenized = [token for token in tokenized if token.isalpha()]\n",
    "    tokenized = [token for token in tokenized if token not in stopwords]\n",
    "    tokenized = [token if not token.isdigit() else \"<NUM>\" for token in tokenized]\n",
    "    encoded = [glove[token].tolist() for token in tokenized if token in glove.stoi]\n",
    "    return encoded\n",
    "\n",
    "# Sample descriptions file format: image_name description\n",
    "descriptions_file = \"filtered_train_descriptions.txt\"\n",
    "preprocessed_data = {}\n",
    "\n",
    "# Load GloVe embeddings\n",
    "glove = GloVe(name='6B', dim=200)\n",
    "stopwords = set(' '.join(torchtext.data.utils.get_tokenizer(\"basic_english\")('')).split())\n",
    "tokenizer = torchtext.data.utils.get_tokenizer(\"basic_english\")\n",
    "\n",
    "# Read descriptions file line by line\n",
    "with open(descriptions_file, 'r') as file:\n",
    "    for line in file:\n",
    "        image_name, description = line.strip().split(maxsplit=1)\n",
    "        preprocessed_data[image_name] = preprocess_description(description, tokenizer, stopwords)\n",
    "\n",
    "# Save preprocessed data to JSON file\n",
    "output_file = \"preprocessed_filtered_train_descriptions.json\"\n",
    "with open(output_file, 'w') as file:\n",
    "    json.dump(preprocessed_data, file)\n",
    "\n",
    "print(\"Preprocessed descriptions saved to:\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: 2401672, Decoded Description: yellow wicker and metal chair sits in corner of deck on boat there are backpacks along the side of the wooden deck shadows are casted on the floor rope ladder is to the left of the deck water surrounds the boat and island land masses are visible in the distance\n",
      "Image: 2378834, Decoded Description: group of people are at the beach two women are sitting in blue beach chairs they have blonde hair pink shirts shorts and are wearing sunglasses man is sitting behind them at picnic table in denim pants and blue and white stripped shirts shirtless boy is pushing bike along side young man four small children play near the shore the two boys are shirtless and wearing shorts and the girls are wearing bathing suits\n",
      "Image: 2349091, Decoded Description: two brown horses are standing together outside there heads are up against the side of the building the both have saddles on they are standing on concrete\n",
      "Image: 2349250, Decoded Description: this is picture of an outdoor scene there is river running horizontally through the picture with grass in the foreground and trees in the background behind the river red footbridge with colorful flags on posts along its length stretches over the bridge on the left side of the image in the foreground is small hooped fence separating the grassy lawn from paved footpath near the center of the image is bench with two people sitting on it man wearing red shirt and woman wearing light blue shirt there are couple of bushes on the grass and tree in the foreground the surface of the river is murky and the bridge is reflected in the surface the sky is overcast\n",
      "Image: 2413944, Decoded Description: people walk in the city on rainy day they were coats hats and carry umbrella to try to keep themselves dry the situation is even more gloomy given the fact that the picture is black and white rather than color we can not see much of the city street as people are blocking the view\n",
      "Image: 2388707, Decoded Description: large man is standing on city sidewalk after nightfall the man is standing close to building with window behind him is piece of furniture for sitting in cars tail lights can be seen as it passes by and two street lamps are shining brightly against the black sky the man is wearing jacket and shirt his hair is dark and wavy\n",
      "Image: 2406250, Decoded Description: large horse barn with many stalls horses have their heads sticking out over the stall doors under each stall door there is green round bucket in front of the stalls is walkway which has wood overhang over it there are people standing along the walkway and metal poles hold the roof up one horse is close and looks directly towards the camera\n",
      "Image: 2318908, Decoded Description: there are bowls on tray each bowl has different foods in it the tray has hello kitty on it on of the bowls have pickles inside of it\n",
      "Image: 2377511, Decoded Description: there is little boy sitting on top of horse the boy is wearing brown hat and holding frame with picture in it people are surrounding the boy watching him wearing robes there is wooden building behind the boy\n",
      "Image: 2402636, Decoded Description: there is stack of hardcover and paperback books on table with an almost empty bottle of corona beer sitting next to them beneath the stack are magazines scattered randomly leaning against the stack is paperback named watchmen white mouse sits behind the books next to green lizard statue behind all these items is white computer monitor that is turned off\n",
      "Image: 2340353, Decoded Description: white truck is driving on street there is white dog in the back of the truck there is black shirt draped over the side of the truck there is tree behind the truck\n",
      "Image: 2391127, Decoded Description: man with black hair is swinging bat he has just hit something filled with red liquid with the blue bat the air is filled with the explosion of red water he is wearing yellow shirt and blue jeans\n",
      "Image: 2327989, Decoded Description: person in yellow green and black jacket and black pants is on an orange purple and yellow snowboard the board is in the air as the rider holds it with his right hand there is dusting of snow below the snowboard and mounds of snow on the ground have tracks in it there is ski lodge behind the rider\n",
      "Image: 2412391, Decoded Description: there is the bottom half of stop light with yellow frame looking down onto two lane road the green light of the stop light is lit it is wet due to previous snowfall as there are sections of snow interspersed with the green grass on the sides of the road the road has two solid yellow lanes and is wet running along the sides of the road there are trees with no leaves and patches of grass there is also driveway on one side and sidewalks running along both sides single black passenger car is heading towards the stop light\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Decode and print descriptions\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image_name, encoded_description \u001b[38;5;129;01min\u001b[39;00m preprocessed_data\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m---> 27\u001b[0m     decoded_description \u001b[38;5;241m=\u001b[39m \u001b[43mdecode_description\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoded_description\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Decoded Description: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdecoded_description\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[16], line 14\u001b[0m, in \u001b[0;36mdecode_description\u001b[1;34m(encoded_description)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m embedding \u001b[38;5;129;01min\u001b[39;00m encoded_description:\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# Find the nearest word in GloVe embeddings\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     embedding_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(embedding)\n\u001b[1;32m---> 14\u001b[0m     distances \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mglove\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvectors\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43membedding_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m     nearest_index \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmin(distances)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     16\u001b[0m     nearest_word \u001b[38;5;241m=\u001b[39m glove\u001b[38;5;241m.\u001b[39mitos[nearest_index]\n",
      "File \u001b[1;32mc:\\Users\\EXTREME_RED\\anaconda3\\envs\\Final_Project_V2\\Lib\\site-packages\\torch\\functional.py:1495\u001b[0m, in \u001b[0;36mnorm\u001b[1;34m(input, p, dim, keepdim, out, dtype)\u001b[0m\n\u001b[0;32m   1489\u001b[0m     \u001b[38;5;129m@overload\u001b[39m  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[0;32m   1490\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnorm\u001b[39m(\u001b[38;5;28minput\u001b[39m, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfro\u001b[39m\u001b[38;5;124m\"\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[0;32m   1491\u001b[0m         \u001b[38;5;66;03m# type: (Tensor, str, Optional[int], bool, Optional[Tensor], Optional[int]) -> Tensor\u001b[39;00m\n\u001b[0;32m   1492\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m-> 1495\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnorm\u001b[39m(\u001b[38;5;28minput\u001b[39m, p: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfro\u001b[39m\u001b[38;5;124m\"\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the matrix norm or vector norm of a given tensor.\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \n\u001b[0;32m   1498\u001b[0m \u001b[38;5;124;03m    .. warning::\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1586\u001b[0m \u001b[38;5;124;03m        (tensor(3.7417), tensor(11.2250))\u001b[39;00m\n\u001b[0;32m   1587\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1589\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28minput\u001b[39m):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "from torchtext.vocab import GloVe\n",
    "\n",
    "# Load GloVe embeddings\n",
    "glove = GloVe(name='6B', dim=200)\n",
    "\n",
    "# Function to decode encoded descriptions\n",
    "def decode_description(encoded_description):\n",
    "    decoded_tokens = []\n",
    "    for embedding in encoded_description:\n",
    "        # Find the nearest word in GloVe embeddings\n",
    "        embedding_tensor = torch.tensor(embedding)\n",
    "        distances = torch.norm(glove.vectors - embedding_tensor, dim=1)\n",
    "        nearest_index = torch.argmin(distances).item()\n",
    "        nearest_word = glove.itos[nearest_index]\n",
    "        decoded_tokens.append(nearest_word)\n",
    "    return ' '.join(decoded_tokens)\n",
    "\n",
    "# Load preprocessed data from JSON file\n",
    "preprocessed_file = \"preprocessed_filtered_train_descriptions.json\"\n",
    "with open(preprocessed_file, 'r') as file:\n",
    "    preprocessed_data = json.load(file)\n",
    "\n",
    "# Decode and print descriptions\n",
    "for image_name, encoded_description in preprocessed_data.items():\n",
    "    decoded_description = decode_description(encoded_description)\n",
    "    print(f\"Image: {image_name}, Decoded Description: {decoded_description}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 4808\n"
     ]
    }
   ],
   "source": [
    "import torchtext\n",
    "import torchtext.data\n",
    "\n",
    "# Path to the descriptions file\n",
    "descriptions_file = \"filtered_train_descriptions.txt\"\n",
    "\n",
    "# Create a counter to count token frequencies\n",
    "counter = torchtext.vocab.Counter()\n",
    "\n",
    "# Load the list of stopwords\n",
    "#stopwords = set(' '.join(torchtext.data.utils.get_tokenizer(\"basic_english\")('')).split())\n",
    "\n",
    "# Read descriptions file line by line and update the counter\n",
    "with open(descriptions_file, 'r') as file:\n",
    "    for line in file:\n",
    "        _, description = line.strip().split(maxsplit=1)\n",
    "        tokens = torchtext.data.utils.get_tokenizer(\"basic_english\")(description.lower())\n",
    "        \n",
    "        # Filter out stopwords from the tokens\n",
    "        #filtered_tokens = [token for token in tokens if token not in stopwords]\n",
    "        #filtered_tokens = [token if not token.isdigit() else \"<NUM>\" for token in tokens]\n",
    "        # Update the counter with filtered tokens\n",
    "        counter.update(tokens)\n",
    "\n",
    "# Create a vocabulary from the counter\n",
    "specials = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "vocab = torchtext.vocab.Vocab(counter, specials=specials)\n",
    "\n",
    "# Save the vocabulary to a file\n",
    "#torchtext.vocab.Vocab.save_vocab(vocab, 'vocabulary.pt')\n",
    "\n",
    "print(\"Vocabulary size:\", len(vocab))\n",
    "#print(\"Vocabulary saved to: vocabulary.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
