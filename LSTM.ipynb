{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 6427595556842017175\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 14048821248\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 9441591463301344886\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 4080, pci bus id: 0000:01:00.0, compute capability: 8.9\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: NVIDIA GeForce RTX 4080, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 6000\n"
     ]
    }
   ],
   "source": [
    "def load_set(filename):\n",
    "\tdoc = load_doc(filename)\n",
    "\tdataset = list()\n",
    "\t# process line by line\n",
    "\tfor line in doc.split('\\n'):\n",
    "\t\t# skip empty lines\n",
    "\t\tif len(line) < 1:\n",
    "\t\t\tcontinue\n",
    "\t\t# get the image identifier\n",
    "\t\tidentifier = line.split('.')[0]\n",
    "\t\tdataset.append(identifier)\n",
    "\treturn set(dataset)\n",
    "\n",
    "# load training dataset (6K)\n",
    "filename = 'Flickr_8k.trainImages.txt'\n",
    "train = load_set(filename)\n",
    "print('Dataset: %d' % len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptions: train=6000\n"
     ]
    }
   ],
   "source": [
    "def load_clean_descriptions(filename, dataset):\n",
    "\t# load document\n",
    "\tdoc = load_doc(filename)\n",
    "\tdescriptions = dict()\n",
    "\tfor line in doc.split('\\n'):\n",
    "\t\t# split line by white space\n",
    "\t\ttokens = line.split()\n",
    "\t\t# split id from description\n",
    "\t\timage_id, image_desc = tokens[0], tokens[1:]\n",
    "\t\t# skip images not in the set\n",
    "\t\tif image_id in dataset:\n",
    "\t\t\t# create list\n",
    "\t\t\tif image_id not in descriptions:\n",
    "\t\t\t\tdescriptions[image_id] = list()\n",
    "\t\t\t# wrap description in tokens\n",
    "\t\t\tdesc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
    "\t\t\t# store\n",
    "\t\t\tdescriptions[image_id].append(desc)\n",
    "\treturn descriptions\n",
    "\n",
    "# descriptions\n",
    "train_descriptions = load_clean_descriptions('descriptions.txt', train)\n",
    "print('Descriptions: train=%d' % len(train_descriptions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_train_captions = []\n",
    "for key, val in train_descriptions.items():\n",
    "    for cap in val:\n",
    "        all_train_captions.append(cap)\n",
    "len(all_train_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessed words 7578 -> 1651\n"
     ]
    }
   ],
   "source": [
    "word_count_threshold = 10\n",
    "word_counts = {}\n",
    "nsents = 0\n",
    "for sent in all_train_captions:\n",
    "    nsents += 1\n",
    "    for w in sent.split(' '):\n",
    "        word_counts[w] = word_counts.get(w, 0) + 1\n",
    "\n",
    "vocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\n",
    "print('preprocessed words %d -> %d' % (len(word_counts), len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ixtoword = {}\n",
    "wordtoix = {}\n",
    "\n",
    "ix = 1\n",
    "for w in vocab:\n",
    "    wordtoix[w] = ix\n",
    "    ixtoword[ix] = w\n",
    "    ix += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'startseq', 2: 'child', 3: 'in', 4: 'pink', 5: 'dress', 6: 'is', 7: 'climbing', 8: 'up', 9: 'set', 10: 'of', 11: 'stairs', 12: 'an', 13: 'way', 14: 'endseq', 15: 'girl', 16: 'going', 17: 'into', 18: 'wooden', 19: 'building', 20: 'little', 21: 'the', 22: 'to', 23: 'her', 24: 'black', 25: 'dog', 26: 'and', 27: 'spotted', 28: 'are', 29: 'fighting', 30: 'tricolored', 31: 'playing', 32: 'with', 33: 'each', 34: 'other', 35: 'on', 36: 'road', 37: 'white', 38: 'brown', 39: 'spots', 40: 'staring', 41: 'at', 42: 'street', 43: 'two', 44: 'dogs', 45: 'different', 46: 'looking', 47: 'pavement', 48: 'moving', 49: 'toward', 50: 'covered', 51: 'paint', 52: 'sits', 53: 'front', 54: 'painted', 55: 'rainbow', 56: 'hands', 57: 'bowl', 58: 'sitting', 59: 'large', 60: 'small', 61: 'grass', 62: 'plays', 63: 'it', 64: 'there', 65: 'pigtails', 66: 'painting', 67: 'young', 68: 'outside', 69: 'man', 70: 'lays', 71: 'bench', 72: 'while', 73: 'his', 74: 'by', 75: 'him', 76: 'which', 77: 'also', 78: 'tied', 79: 'sleeping', 80: 'next', 81: 'shirtless', 82: 'lies', 83: 'park', 84: 'laying', 85: 'holding', 86: 'leash', 87: 'ground', 88: 'orange', 89: 'hat', 90: 'something', 91: 'wears', 92: 'glasses', 93: 'wearing', 94: 'beer', 95: 'can', 96: 'ears', 97: 'rope', 98: 'net', 99: 'red', 100: 'climbs', 101: 'bridge', 102: 'onto', 103: 'ropes', 104: 'playground', 105: 'running', 106: 'grassy', 107: 'garden', 108: 'surrounded', 109: 'fence', 110: 'through', 111: 'terrier', 112: 'green', 113: 'runs', 114: 'near', 115: 'shakes', 116: 'its', 117: 'head', 118: 'shore', 119: 'ball', 120: 'edge', 121: 'beach', 122: 'feet', 123: 'stands', 124: 'shaking', 125: 'off', 126: 'water', 127: 'standing', 128: 'turned', 129: 'one', 130: 'side', 131: 'boy', 132: 'smiles', 133: 'wall', 134: 'city', 135: 'overalls', 136: 'working', 137: 'stone', 138: 'walking', 139: 'paved', 140: 'metal', 141: 'pole', 142: 'behind', 143: 'smiling', 144: 'shirt', 145: 'blue', 146: 'jeans', 147: 'rock', 148: 'leaps', 149: 'over', 150: 'log', 151: 'grey', 152: 'leaping', 153: 'fallen', 154: 'tree', 155: 'collar', 156: 'jumping', 157: 'jumped', 158: 'stump', 159: 'snow', 160: 'field', 161: 'surface', 162: 'pictures', 163: 'skier', 164: 'skis', 165: 'past', 166: 'another', 167: 'person', 168: 'looks', 169: 'trees', 170: 'for', 171: 'cliff', 172: 'group', 173: 'people', 174: 'seven', 175: 'climbers', 176: 'face', 177: 'whilst', 178: 'several', 179: 'row', 180: 'watches', 181: 'holds', 182: 'line', 183: 'chases', 184: 'from', 185: 'sprinkler', 186: 'lawn', 187: 'hose', 188: 'away', 189: 'prepares', 190: 'catch', 191: 'thrown', 192: 'object', 193: 'nearby', 194: 'cars', 195: 'about', 196: 'yellow', 197: 'mouth', 198: 'toy', 199: 'ready', 200: 'flying', 201: 'air', 202: 'after', 203: 'get', 204: 'jumps', 205: 'towards', 206: 'trying', 207: 'midair', 208: 'couple', 209: 'infant', 210: 'being', 211: 'held', 212: 'male', 213: 'pond', 214: 'stroller', 215: 'sit', 216: 'baby', 217: 'their', 218: 'under', 219: 'facing', 220: 'lake', 221: 'woman', 222: 'along', 223: 'body', 224: 'outdoors', 225: 'surf', 226: 'lab', 227: 'splashes', 228: 'this', 229: 'splashing', 230: 'hole', 231: 'ice', 232: 'frozen', 233: 'men', 234: 'fishing', 235: 'play', 236: 'making', 237: 'turn', 238: 'sand', 239: 'together', 240: 'tan', 241: 'sandy', 242: 'uses', 243: 'climber', 244: 'jacket', 245: 'pants', 246: 'scaling', 247: 'waterfall', 248: 'carries', 249: 'as', 250: 'he', 251: 'walks', 252: 'carrying', 253: 'has', 254: 'item', 255: 'wet', 256: 'kayak', 257: 'life', 258: 'jackets', 259: 'rowing', 260: 'canoe', 261: 'waters', 262: 'ride', 263: 'courtyard', 264: 'catching', 265: 'chocolate', 266: 'driveway', 267: 'art', 268: 'structure', 269: 'glass', 270: 'reads', 271: 'newspaper', 272: 'sculpture', 273: 'office', 274: 'statue', 275: 'backpack', 276: 'buildings', 277: 'reading', 278: 'tent', 279: 'setting', 280: 'snowy', 281: 'very', 282: 'tall', 283: 'distance', 284: 'camera', 285: 'mountainside', 286: 'out', 287: 'view', 288: 'three', 289: 'overlooking', 290: 'valley', 291: 'hang', 292: 'top', 293: 'big', 294: 'hill', 295: 'rest', 296: 'ledge', 297: 'above', 298: 'down', 299: 'many', 300: 'inflatable', 301: 'boats', 302: 'railing', 303: 'below', 304: 'crowd', 305: 'jersey', 306: 'pose', 307: 'hand', 308: 'some', 309: 'posing', 310: 'picture', 311: 'asian', 312: 'blond', 313: 'background', 314: 'guy', 315: 'biting', 316: 'striped', 317: 'rail', 318: 'crowded', 319: 'takes', 320: 'jump', 321: 'skateboard', 322: 'performing', 323: 'trick', 324: 'leans', 325: 'skateboarder', 326: 'doing', 327: 'board', 328: 'platform', 329: 'skateboarders', 330: 'paddling', 331: 'river', 332: 'seen', 333: 'kayaking', 334: 'paddles', 335: 'boat', 336: 'paddle', 337: 'shallow', 338: 'girls', 339: 'ocean', 340: 'four', 341: 'children', 342: 'pajamas', 343: 'have', 344: 'pillow', 345: 'fight', 346: 'kids', 347: 'bed', 348: 'having', 349: 'workers', 350: 'beam', 351: 'taking', 352: 'break', 353: 'construction', 354: 'take', 355: 'seat', 356: 'train', 357: 'rides', 358: 'book', 359: 'rocky', 360: 'run', 361: 'across', 362: 'area', 363: 'end', 364: 'high', 365: 'diving', 366: 'pool', 367: 'kid', 368: 'swimming', 369: 'window', 370: 'tank', 371: 'door', 372: 'puts', 373: 'putting', 374: 'light', 375: 'hiker', 376: 'mountains', 377: 'ski', 378: 'landscape', 379: 'snowcovered', 380: 'mountain', 381: 'beautiful', 382: 'mountaintop', 383: 'attempting', 384: 'purple', 385: 'low', 386: 'cut', 387: 'yard', 388: 'frisbee', 389: 'shorts', 390: 'parking', 391: 'lot', 392: 'middle', 393: 'during', 394: 'heavy', 395: 'traffic', 396: 'mat', 397: 'between', 398: 'open', 399: 'busy', 400: 'terrain', 401: 'doberman', 402: 'chasing', 403: 'catches', 404: 'tennis', 405: 'watching', 406: 'balloons', 407: 'night', 408: 'hot', 409: 'lit', 410: 'lined', 411: 'helmet', 412: 'bike', 413: 'dirt', 414: 'bicycle', 415: 'race', 416: 'quickly', 417: 'bmx', 418: 'dark', 419: 'throwing', 420: 'bubbles', 421: 'foam', 422: 'ramp', 423: 'getting', 424: 'sticks', 425: 'tongue', 426: 'boys', 427: 'make', 428: 'faces', 429: 'sticking', 430: 'look', 431: 'silly', 432: 'blonde', 433: 'horse', 434: 'sweatshirt', 435: 'fire', 436: 'barrel', 437: 'stand', 438: 'lead', 439: 'horses', 440: 'sheep', 441: 'pushing', 442: 'skinny', 443: 'smaller', 444: 'them', 445: 'various', 446: 'safety', 447: 'harness', 448: 'indoor', 449: 'rocks', 450: 'ring', 451: 'teenage', 452: 'slide', 453: 'forest', 454: 'skiing', 455: 'wooded', 456: 'crosscountry', 457: 'skiers', 458: 'woodland', 459: 'sliding', 460: 'trail', 461: 'woods', 462: 'lone', 463: 'hikers', 464: 'pathway', 465: 'path', 466: 'happily', 467: 'dressed', 468: 'atop', 469: 'pull', 470: 'cart', 471: 'golden', 472: 'coat', 473: 'pulling', 474: 'carriage', 475: 'sled', 476: 'sheer', 477: 'using', 478: 'flat', 479: 'steep', 480: 'vest', 481: 'inside', 482: 'cave', 483: 'shows', 484: 'sunlight', 485: 'lay', 486: 'dry', 487: 'resting', 488: 'gear', 489: 'chalk', 490: 'stream', 491: 'drawing', 492: 'sidewalk', 493: 'deep', 494: 'pile', 495: 'fountain', 496: 'fountains', 497: 'sprayed', 498: 'tunnel', 499: 'course', 500: 'obstacle', 501: 'around', 502: 'swimsuit', 503: 'arms', 504: 'outstretched', 505: 'handrail', 506: 'bathing', 507: 'suit', 508: 'drinking', 509: 'spray', 510: 'soaked', 511: 'jet', 512: 'shower', 513: 'gets', 514: 'underwater', 515: 'redheaded', 516: 'swims', 517: 'alone', 518: 'snowmobile', 519: 'flies', 520: 'rider', 521: 'or', 522: 'machine', 523: 'pine', 524: 'rural', 525: 'riding', 526: 'helmets', 527: 'goggles', 528: 'drive', 529: 'heads', 530: 'atv', 531: 'wheel', 532: 'empty', 533: 'all', 534: 'vehicle', 535: 'airborne', 536: 'soccer', 537: 'arm', 538: 'tshirt', 539: 'artist', 540: 'clouds', 541: 'colors', 542: 'paper', 543: 'cyclist', 544: 'sharp', 545: 'curve', 546: 'cows', 547: 'biker', 548: 'old', 549: 'video', 550: 'lips', 551: 'gun', 552: 'collie', 553: 'audience', 554: 'watch', 555: 'onlookers', 556: 'toddler', 557: 'shoes', 558: 'attached', 559: 'that', 560: 'public', 561: 'wood', 562: 'animal', 563: 'bird', 564: 'eating', 565: 'someone', 566: 'eats', 567: 'finger', 568: 'fast', 569: 'wades', 570: 'laughs', 571: 'colorful', 572: 'outdoor', 573: 'handstand', 574: 'does', 575: 'upside', 576: 'snowboard', 577: 'surfboard', 578: 'bikes', 579: 'traveling', 580: 'motorcycles', 581: 'half', 582: 'naked', 583: 'chair', 584: 'older', 585: 'back', 586: 'relaxes', 587: 'patio', 588: 'where', 589: 'bicycles', 590: 'cap', 591: 'deck', 592: 'close', 593: 'parked', 594: 'relaxing', 595: 'topless', 596: 'floating', 597: 'both', 598: 'mouths', 599: 'tug', 600: 'chew', 601: 'haired', 602: 'drink', 603: 'hair', 604: 'party', 605: 'streets', 606: 'they', 607: 'women', 608: 'parade', 609: 'splash', 610: 'sandals', 611: 'short', 612: 'backpacks', 613: 'cardboard', 614: 'bus', 615: 'station', 616: 'room', 617: 'plants', 618: 'crossing', 619: 'sign', 620: 'beside', 621: 'family', 622: 'gathered', 623: 'van', 624: 'bright', 625: 'truck', 626: 'others', 627: 'helping', 628: 'step', 629: 'pulled', 630: 'brightly', 631: 'poses', 632: 'costume', 633: 'hugs', 634: 'who', 635: 'event', 636: 'hugging', 637: 'bicyclist', 638: 'cross', 639: 'biking', 640: 'lady', 641: 'sky', 642: 'car', 643: 'wait', 644: 'jogging', 645: 'jogs', 646: 'headphones', 647: 'plant', 648: 'store', 649: 'corner', 650: 'bicyclists', 651: 'intersection', 652: 'waiting', 653: 'bikers', 654: 'stop', 655: 'town', 656: 'without', 657: 'wagon', 658: 'leading', 659: 'hits', 660: 'hitting', 661: 'baseball', 662: 'adult', 663: 'put', 664: 'cage', 665: 'spins', 666: 'waves', 667: 'sun', 668: 'she', 669: 'marching', 670: 'band', 671: 'teenager', 672: 'game', 673: 'sneakers', 674: 'leap', 675: 'elderly', 676: 'straw', 677: 'gray', 678: 'sweater', 679: 'walk', 680: 'hallway', 681: 'colored', 682: 'flip', 683: 'flops', 684: 'hood', 685: 'legs', 686: 'boulder', 687: 'clear', 688: 'hooded', 689: 'bending', 690: 'blanket', 691: 'stuffed', 692: 'animals', 693: 'giving', 694: 'toys', 695: 'war', 696: 'floor', 697: 'nose', 698: 'silver', 699: 'closeup', 700: 'bar', 701: 'neon', 702: 'hanging', 703: 'bank', 704: 'computer', 705: 'rough', 706: 'shaped', 707: 'plastic', 708: 'team', 709: 'wear', 710: 'uniforms', 711: 'same', 712: 'hats', 713: 'caps', 714: 'flowers', 715: 'shaggy', 716: 'stick', 717: 'long', 718: 'alongside', 719: 'advertisement', 720: 'subway', 721: 'umbrella', 722: 'unicycle', 723: 'scooter', 724: 'reaches', 725: 'dock', 726: 'pier', 727: 'concrete', 728: 'landing', 729: 'table', 730: 'picnic', 731: 'luggage', 732: 'eat', 733: 'floral', 734: 'purse', 735: 'suspended', 736: 'type', 737: 'strange', 738: 'eyes', 739: 'closed', 740: 'photo', 741: 'outfit', 742: 'photograph', 743: 'just', 744: 'church', 745: 'climb', 746: 'swing', 747: 'swings', 748: 'heels', 749: 'brick', 750: 'against', 751: 'wrestle', 752: 'plain', 753: 'clothes', 754: 'asleep', 755: 'sofa', 756: 'pacifier', 757: 'bear', 758: 'soda', 759: 'vendor', 760: 'drinks', 761: 'print', 762: 'gold', 763: 'stove', 764: 'pipe', 765: 'makeup', 766: 'benches', 767: 'sunglasses', 768: 'mask', 769: 'wetsuit', 770: 'bald', 771: 'flips', 772: 'ear', 773: 'bite', 774: 'number', 775: 'six', 776: 'chased', 777: 'kneeling', 778: 'approaching', 779: 'thin', 780: 'carpet', 781: 'rug', 782: 'lying', 783: 'furry', 784: 'suits', 785: 'friends', 786: 'smile', 787: 'ladies', 788: 'bikinis', 789: 'reflection', 790: 'dances', 791: 'skirt', 792: 'crouches', 793: 'writing', 794: 'blurry', 795: 'descending', 796: 'kicking', 797: 'police', 798: 'motorcycle', 799: 'muddy', 800: 'motorbike', 801: 'uphill', 802: 'motocross', 803: 'racer', 804: 'display', 805: 'underwear', 806: 'wire', 807: 'creek', 808: 'leaning', 809: 'motion', 810: 'says', 811: 'sunset', 812: 'float', 813: 'drives', 814: 'waits', 815: 'driving', 816: 'lap', 817: 'tries', 818: 'playpen', 819: 'female', 820: 'football', 821: 'foot', 822: 'first', 823: 'seated', 824: 'candles', 825: 'birthday', 826: 'place', 827: 'cake', 828: 'trotting', 829: 'few', 830: 'adults', 831: 'splashed', 832: 'sunny', 833: 'day', 834: 'lots', 835: 'shown', 836: 'cloudy', 837: 'khaki', 838: 'passes', 839: 'sideways', 840: 'waterskier', 841: 'performs', 842: 'tricks', 843: 'waterskiing', 844: 'knee', 845: 'begins', 846: 'fall', 847: 'wakeboarding', 848: 'younger', 849: 'fish', 850: 'stare', 851: 'pitch', 852: 'uniform', 853: 'player', 854: 'backyard', 855: 'stuck', 856: 'turning', 857: 'wide', 858: 'barks', 859: 'shoulder', 860: 'fluffy', 861: 'grab', 862: 'tables', 863: 'talks', 864: 'showing', 865: 'talking', 866: 'phone', 867: 'cellphone', 868: 'flower', 869: 'attire', 870: 'device', 871: 'american', 872: 'flag', 873: 'hiking', 874: 'hikes', 875: 'underneath', 876: 'stool', 877: 'chairs', 878: 'gives', 879: 'eye', 880: 'trunks', 881: 'reflective', 882: 'swim', 883: 'block', 884: 'dirty', 885: 'bottle', 886: 'checkered', 887: 'tight', 888: 'bars', 889: 'hangs', 890: 'monkey', 891: 'jungle', 892: 'gym', 893: 'ladder', 894: 'warm', 895: 'swinging', 896: 'wings', 897: 'slides', 898: 'floaties', 899: 'hit', 900: 'jean', 901: 'railroad', 902: 'tracks', 903: 'barefoot', 904: 'bearded', 905: 'peace', 906: 'stops', 907: 'feather', 908: 'shaved', 909: 'shopping', 910: 'filled', 911: 'wave', 912: 'sports', 913: 'attempt', 914: 'five', 915: 'tire', 916: 'racers', 917: 'racing', 918: 'laughing', 919: 'bucket', 920: 'beneath', 921: 'boogie', 922: 'trampoline', 923: 'puppies', 924: 'chase', 925: 'heavily', 926: 'cold', 927: 'but', 928: 'sets', 929: 'balloon', 930: 'figure', 931: 'right', 932: 'slightly', 933: 'floats', 934: 'bikini', 935: 'huge', 936: 'scene', 937: 'skimpy', 938: 'foreground', 939: 'skating', 940: 'helps', 941: 'skate', 942: 'hold', 943: 'go', 944: 'rink', 945: 'museum', 946: 'retriever', 947: 'kissing', 948: 'cheek', 949: 'tie', 950: 'kiss', 951: 'happy', 952: 'shirts', 953: 'vests', 954: 'formal', 955: 'balcony', 956: 'amusement', 957: 'carnival', 958: 'swords', 959: 'bat', 960: 'cup', 961: 'rubber', 962: 'referee', 963: 'funny', 964: 'bushes', 965: 'matching', 966: 'denim', 967: 'gravel', 968: 'land', 969: 'puppy', 970: 'beige', 971: 'left', 972: 'frame', 973: 'help', 974: 'push', 975: 'among', 976: 'cement', 977: 'shop', 978: 'retrieves', 979: 'golf', 980: 'hind', 981: 'taken', 982: 'balls', 983: 'no', 984: 'fetch', 985: 'spectators', 986: 'pitbull', 987: 'spraying', 988: 'jack', 989: 'marker', 990: 'house', 991: 'mirror', 992: 'hoodie', 993: 'softball', 994: 'throw', 995: 'playfully', 996: 'tossing', 997: 'throws', 998: 'marked', 999: 'track', 1000: 'circle', 1001: 'camping', 1002: 'equipment', 1003: 'bags', 1004: 'bottom', 1005: 'base', 1006: 'surfing', 1007: 'made', 1008: 'dune', 1009: 'snowboarding', 1010: 'poodle', 1011: 'full', 1012: 'bath', 1013: 'cloth', 1014: 'wading', 1015: 'towel', 1016: 'formation', 1017: 'walkway', 1018: 'enjoying', 1019: 'wine', 1020: 'screen', 1021: 'laptop', 1022: 'snowboarder', 1023: 'cover', 1024: 'sheet', 1025: 'females', 1026: 'clothing', 1027: 'case', 1028: 'outfits', 1029: 'dresses', 1030: 'fishes', 1031: 'tops', 1032: 'these', 1033: 'you', 1034: 'plaid', 1035: 'bag', 1036: 'speaks', 1037: 'coffee', 1038: 'restaurant', 1039: 'be', 1040: 'only', 1041: 'retrieving', 1042: 'pack', 1043: 'father', 1044: 'peak', 1045: 'like', 1046: 'cowboy', 1047: 'chain', 1048: 'neck', 1049: 'roller', 1050: 'coaster', 1051: 'shot', 1052: 'poles', 1053: 'slope', 1054: 'innertube', 1055: 'tube', 1056: 'boots', 1057: 'brunette', 1058: 'curb', 1059: 'lift', 1060: 'rolling', 1061: 'competing', 1062: 'agility', 1063: 'palm', 1064: 'touch', 1065: 'gentleman', 1066: 'lean', 1067: 'second', 1068: 'themselves', 1069: 'waist', 1070: 'pirate', 1071: 'patch', 1072: 'bandanna', 1073: 'makes', 1074: 'headscarf', 1075: 'catcher', 1076: 'points', 1077: 'pointing', 1078: 'sides', 1079: 'teams', 1080: 'players', 1081: 'volleyball', 1082: 'athletic', 1083: 'stares', 1084: 'preparing', 1085: 'forward', 1086: 'necklace', 1087: 'trunk', 1088: 'weather', 1089: 'guys', 1090: 'touches', 1091: 'brother', 1092: 'reaching', 1093: 'gloves', 1094: 'food', 1095: 'speaking', 1096: 'box', 1097: 'hurdle', 1098: 'pulls', 1099: 'cigarette', 1100: 'fisherman', 1101: 'mohawk', 1102: 'style', 1103: 'shade', 1104: 'ribbon', 1105: 'karate', 1106: 'martial', 1107: 'arts', 1108: 'practicing', 1109: 'enjoys', 1110: 'german', 1111: 'piece', 1112: 'handles', 1113: 'touching', 1114: 'graffiti', 1115: 'skateboarding', 1116: 'skater', 1117: 'amidst', 1118: 'cloud', 1119: 'kitchen', 1120: 'meal', 1121: 'spread', 1122: 'tattoos', 1123: 'backs', 1124: 'bathroom', 1125: 'mother', 1126: 'feeding', 1127: 'watched', 1128: 'raising', 1129: 'almost', 1130: 'partially', 1131: 'shoulders', 1132: 'beard', 1133: 'disc', 1134: 'try', 1135: 'attempts', 1136: 'distant', 1137: 'scales', 1138: 'fun', 1139: 'multicolored', 1140: 'branch', 1141: 'chewing', 1142: 'leafy', 1143: 'teeth', 1144: 'bleachers', 1145: 'third', 1146: 'teen', 1147: 'school', 1148: 'gate', 1149: 'crashing', 1150: 'rapids', 1151: 'cyclists', 1152: 'bottles', 1153: 'branches', 1154: 'summer', 1155: 'vehicles', 1156: 'bites', 1157: 'steps', 1158: 'bouncing', 1159: 'enclosed', 1160: 'balancing', 1161: 'leg', 1162: 'see', 1163: 'gestures', 1164: 'though', 1165: 'emerges', 1166: 'kayaker', 1167: 'goes', 1168: 'rain', 1169: 'jeep', 1170: 'training', 1171: 'pale', 1172: 'downhill', 1173: 'court', 1174: 'visible', 1175: 'counter', 1176: 'murky', 1177: 'rafting', 1178: 'tires', 1179: 'leaves', 1180: 'longhaired', 1181: 'bounds', 1182: 'skateboards', 1183: 'do', 1184: 'mud', 1185: 'crosswalk', 1186: 'not', 1187: 'so', 1188: 'narrow', 1189: 'hills', 1190: 'autumn', 1191: 'bare', 1192: 'scuba', 1193: 'diver', 1194: 'coats', 1195: 'ahead', 1196: 'petting', 1197: 'farm', 1198: 'fenced', 1199: 'pen', 1200: 'goat', 1201: 'curly', 1202: 'upsidedown', 1203: 'flipping', 1204: 'gather', 1205: 'wrestling', 1206: 'horizon', 1207: 'kite', 1208: 'countryside', 1209: 'fly', 1210: 'wheelie', 1211: 'homeless', 1212: 'couch', 1213: 'scarf', 1214: 'beads', 1215: 'slip', 1216: 'barren', 1217: 'skull', 1218: 'dust', 1219: 'blowing', 1220: 'wind', 1221: 'urban', 1222: 'digging', 1223: 'brush', 1224: 'kneels', 1225: 'string', 1226: 'rests', 1227: 'pouring', 1228: 'hay', 1229: 'fur', 1230: 'listening', 1231: 'fair', 1232: 'image', 1233: 'covering', 1234: 'mound', 1235: 'raised', 1236: 'raises', 1237: 'cricket', 1238: 'singing', 1239: 'sings', 1240: 'microphone', 1241: 'guitar', 1242: 'crawls', 1243: 'knees', 1244: 'snowball', 1245: 'fabric', 1246: 'kicks', 1247: 'teammate', 1248: 'puddle', 1249: 'opposite', 1250: 'lights', 1251: 'corn', 1252: 'photographer', 1253: 'driver', 1254: 'ridden', 1255: 'professional', 1256: 'aqua', 1257: 'country', 1258: 'flags', 1259: 'boardwalk', 1260: 'link', 1261: 'silhouette', 1262: 'rows', 1263: 'students', 1264: 'sniffs', 1265: 'sniffing', 1266: 'hoop', 1267: 'motorcyclist', 1268: 'speed', 1269: 'racetrack', 1270: 'position', 1271: 'sea', 1272: 'camouflage', 1273: 'bunch', 1274: 'picking', 1275: 'surfer', 1276: 'surfs', 1277: 'foggy', 1278: 'dreadlocks', 1279: 'flowered', 1280: 'staircase', 1281: 'embrace', 1282: 'goal', 1283: 'goalie', 1284: 'uniformed', 1285: 'fake', 1286: 'races', 1287: 'balances', 1288: 'balance', 1289: 'himself', 1290: 'tricycle', 1291: 'following', 1292: 'hike', 1293: 'tag', 1294: 'stomach', 1295: 'headfirst', 1296: 'shoreline', 1297: 'cat', 1298: 'stripes', 1299: 'swimmer', 1300: 'skinned', 1301: 'neighborhood', 1302: 'larger', 1303: 'living', 1304: 'leashes', 1305: 'reach', 1306: 'costumes', 1307: 'music', 1308: 'sprinklers', 1309: 'logs', 1310: 'itself', 1311: 'single', 1312: 'time', 1313: 'spiderman', 1314: 'candy', 1315: 'polka', 1316: 'dot', 1317: 'hardhat', 1318: 'carry', 1319: 'coming', 1320: 'stadium', 1321: 'licking', 1322: 'medium', 1323: 'sized', 1324: 'lounge', 1325: 'cream', 1326: 'follows', 1327: 'owner', 1328: 'pit', 1329: 'still', 1330: 'plate', 1331: 'dead', 1332: 'winter', 1333: 'muzzle', 1334: 'blow', 1335: 'hoops', 1336: 'arcade', 1337: 'home', 1338: 'wrapped', 1339: 'work', 1340: 'caught', 1341: 'spinning', 1342: 'turns', 1343: 'appears', 1344: 'markings', 1345: 'basketball', 1346: 'rollerblading', 1347: 'inline', 1348: 'rollerblades', 1349: 'skates', 1350: 'enjoy', 1351: 'basket', 1352: 'desert', 1353: 'space', 1354: 'porch', 1355: 'hillside', 1356: 'foliage', 1357: 'indoors', 1358: 'teenagers', 1359: 'talk', 1360: 'moves', 1361: 'stage', 1362: 'onstage', 1363: 'perform', 1364: 'snowsuit', 1365: 'licks', 1366: 'poodles', 1367: 'raft', 1368: 'parachute', 1369: 'gathering', 1370: 'duck', 1371: 'collars', 1372: 'cape', 1373: 'greyhound', 1374: 'greyhounds', 1375: 'dusk', 1376: 'surrounding', 1377: 'decorated', 1378: 'riders', 1379: 'hug', 1380: 'christmas', 1381: 'headband', 1382: 'friend', 1383: 'starting', 1384: 'money', 1385: 'market', 1386: 'grocery', 1387: 'direction', 1388: 'range', 1389: 'casting', 1390: 'chest', 1391: 'protective', 1392: 'identical', 1393: 'breaking', 1394: 'icy', 1395: 'muzzled', 1396: 'buckets', 1397: 'huddle', 1398: 'falling', 1399: 'tail', 1400: 'rear', 1401: 'broken', 1402: 'army', 1403: 'knit', 1404: 'shepherd', 1405: 'cow', 1406: 'bull', 1407: 'ponytail', 1408: 'what', 1409: 'rolls', 1410: 'paws', 1411: 'snowcapped', 1412: 'crouching', 1413: 'giant', 1414: 'before', 1415: 'toilet', 1416: 'action', 1417: 'pitcher', 1418: 'pigeons', 1419: 'birds', 1420: 'barrier', 1421: 'laugh', 1422: 'wheels', 1423: 'mostly', 1424: 'swimmers', 1425: 'swimsuits', 1426: 'smoking', 1427: 'crossed', 1428: 'herself', 1429: 'dives', 1430: 'santa', 1431: 'african', 1432: 'desk', 1433: 'parka', 1434: 'rowboat', 1435: 'roof', 1436: 'worker', 1437: 'cheerleaders', 1438: 'waving', 1439: 'stunt', 1440: 'move', 1441: 'flock', 1442: 'pair', 1443: 'grinding', 1444: 'booth', 1445: 'barn', 1446: 'signs', 1447: 'backwards', 1448: 'photographed', 1449: 'pass', 1450: 'business', 1451: 'guard', 1452: 'officer', 1453: 'bow', 1454: 'target', 1455: 'leads', 1456: 'hurdles', 1457: 'dancing', 1458: 'fancy', 1459: 'violin', 1460: 'doorway', 1461: 'eastern', 1462: 'tiger', 1463: 'tattoo', 1464: 'indian', 1465: 'native', 1466: 'hockey', 1467: 'surfers', 1468: 'airplane', 1469: 'cones', 1470: 'cone', 1471: 'flames', 1472: 'fingers', 1473: 'leaving', 1474: 'muzzles', 1475: 'sledding', 1476: 'shoveling', 1477: 'shovel', 1478: 'japanese', 1479: 'class', 1480: 'passing', 1481: 'instrument', 1482: 'mug', 1483: 'smokes', 1484: 'square', 1485: 'dance', 1486: 'meadow', 1487: 'members', 1488: 'umbrellas', 1489: 'boxing', 1490: 'sleeps', 1491: 'kisses', 1492: 'leather', 1493: 'cats', 1494: 'blows', 1495: 'barking', 1496: 'speeds', 1497: 'wedding', 1498: 'bride', 1499: 'comes', 1500: 'grabbing', 1501: 'dribbles', 1502: 'robe', 1503: 'hard', 1504: 'horseback', 1505: 'colourful', 1506: 'fetching', 1507: 'crosses', 1508: 'drums', 1509: 'instruments', 1510: 'drum', 1511: 'bounce', 1512: 'shadow', 1513: 'kick', 1514: 'straight', 1515: 'glove', 1516: 'batman', 1517: 'fireplace', 1518: 'wrestler', 1519: 'bush', 1520: 'natural', 1521: 'digs', 1522: 'part', 1523: 'alley', 1524: 'played', 1525: 'rollerskating', 1526: 'been', 1527: 'railings', 1528: 'musicians', 1529: 'pet', 1530: 'flute', 1531: 'falls', 1532: 'point', 1533: 'robes', 1534: 'pushes', 1535: 'competition', 1536: 'numbered', 1537: 'thumbs', 1538: 'racquet', 1539: 'racket', 1540: 'ducks', 1541: 'geese', 1542: 'either', 1543: 'ship', 1544: 'skyline', 1545: 'bend', 1546: 'round', 1547: 'match', 1548: 'plaza', 1549: 'apron', 1550: 'tulips', 1551: 'oklahoma', 1552: 'sooners', 1553: 'tackle', 1554: 'coach', 1555: 'strip', 1556: 'fans', 1557: 'tackled', 1558: 'jerseys', 1559: 'tackling', 1560: 'cheerleader', 1561: 'blocks', 1562: 'opposing', 1563: 'center', 1564: 'compete', 1565: 'appear', 1566: 'runners', 1567: 'crying', 1568: 'flight', 1569: 'bathtub', 1570: 'monument', 1571: 'protest', 1572: 'hospital', 1573: 'cast', 1574: 'gallery', 1575: 'wheelchair', 1576: 'show', 1577: 'jumper', 1578: 'jockey', 1579: 'blurred', 1580: 'cafe', 1581: 'banner', 1582: 'how', 1583: 'merrygoround', 1584: 'tripod', 1585: 'photographs', 1586: 'camel', 1587: 'shooting', 1588: 'bubble', 1589: 'ribbons', 1590: 'sing', 1591: 'hula', 1592: 'advertising', 1593: 'smoke', 1594: 'trashcan', 1595: 'skirts', 1596: 'followed', 1597: 'tutu', 1598: 'boarding', 1599: 'aged', 1600: 'apple', 1601: 'beagle', 1602: 'well', 1603: 'shoot', 1604: 'shoe', 1605: 'bunny', 1606: 'tents', 1607: 'traditional', 1608: 'control', 1609: 'crouched', 1610: 'tuxedos', 1611: 'gliding', 1612: 'sail', 1613: 'chews', 1614: 'clown', 1615: 'kiddie', 1616: 'athlete', 1617: 'rollerblader', 1618: 'grinds', 1619: 'sport', 1620: 'rugby', 1621: 'policeman', 1622: 'rodeo', 1623: 'stretches', 1624: 'stair', 1625: 'ridge', 1626: 'runner', 1627: 'accordion', 1628: 'officers', 1629: 'miami', 1630: 'crane', 1631: 'houses', 1632: 'eight', 1633: 'puck', 1634: 'obama', 1635: 'bone', 1636: 'swan', 1637: 'plane', 1638: 'jockeys', 1639: 'masks', 1640: 'groom', 1641: 'mural', 1642: 'daughter', 1643: 'deer', 1644: 'scarves', 1645: 'fruit', 1646: 'military', 1647: 'elephant', 1648: 'wrestlers', 1649: 'cards', 1650: 'sumo', 1651: 'camels'}\n"
     ]
    }
   ],
   "source": [
    "print(ixtoword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1652"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(ixtoword) + 1 # one for appended 0's\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description Length: 34\n"
     ]
    }
   ],
   "source": [
    "def to_lines(descriptions):\n",
    "\tall_desc = list()\n",
    "\tfor key in descriptions.keys():\n",
    "\t\t[all_desc.append(d) for d in descriptions[key]]\n",
    "\treturn all_desc\n",
    "\n",
    "# calculate the length of the description with the most words\n",
    "def max_length(descriptions):\n",
    "\tlines = to_lines(descriptions)\n",
    "\treturn max(len(d.split()) for d in lines)\n",
    "\n",
    "# determine the maximum sequence length\n",
    "max_length = max_length(train_descriptions)\n",
    "print('Description Length: %d' % max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# Load Glove vectors\n",
    "glove_dir = 'archive'\n",
    "embeddings_index = {} # empty dictionary\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.200d.txt'), encoding=\"utf-8\")\n",
    "\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 200\n",
    "\n",
    "# Get 200-dim dense vector for each of the 10000 words in out vocabulary\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "for word, i in wordtoix.items():\n",
    "    #if i < max_words:\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in the embedding index will be all zeros\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1652, 200)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.32927999  0.25525999  0.26752999 -0.084809    0.29764     0.062339\n",
      " -0.15475     0.17783999  0.32328001 -0.92751998  0.15194     0.16324\n",
      " -0.10428    -0.026464    0.65970999  0.14782     0.38622999  0.25169\n",
      "  0.1261     -0.43138     0.28092     3.16039991 -0.17565    -0.0032247\n",
      "  0.64389002 -0.39697     0.18975     0.37999001 -0.079175   -0.14781\n",
      " -0.072965    0.057247   -0.42313999  0.4508     -0.097386   -0.47587001\n",
      " -0.96599001 -0.75594997 -0.033932   -0.070886   -0.44828001 -0.52094001\n",
      " -0.1823      0.18582    -0.074273   -0.017871    0.16742     0.015459\n",
      "  0.30289999 -0.1258      0.32418001 -0.31263    -0.076832    0.051959\n",
      "  0.27241999 -0.18285    -0.36478999 -0.63562    -0.21685     0.035812\n",
      "  0.12485     0.37268001 -0.16976    -0.094146   -0.16412    -0.10728\n",
      "  0.037866    0.1175     -0.15533     0.34062001  0.58848     0.38992\n",
      " -0.54838997  0.85013002 -0.83727998  0.15482    -0.37191001 -0.65408999\n",
      " -0.27631    -0.025224    0.075732   -0.23904    -0.18311    -0.084571\n",
      "  0.15492    -0.16316999 -0.26499     0.056831    0.88287002 -0.47655001\n",
      "  0.25130999 -0.09316     0.34377    -0.35863    -0.22855     0.11918\n",
      "  0.29661    -0.2536      0.049002   -0.21234     0.16237     0.53871\n",
      "  0.035344    0.39293    -0.29673001 -0.72556001 -0.27430999  1.34689999\n",
      " -0.19217999  0.50533998  0.028451   -0.32205999  0.096035   -0.0083551\n",
      " -0.013107   -0.32444    -0.10163     0.031755   -0.63195997 -0.21540999\n",
      " -0.035609    0.31259     0.23988    -0.19056    -0.13086    -0.12644\n",
      "  0.48795    -0.13492    -0.41966999  0.15904    -0.27921    -0.017258\n",
      "  0.29370001  0.067436    0.085052    0.099394   -0.0055281   0.094985\n",
      "  0.11167     0.19749001  0.25229999  0.32205001  0.42778    -0.03518\n",
      "  1.32910001  0.005261    0.26769    -0.46168     0.1125      0.10111\n",
      " -0.31174001  0.54579997 -0.37362999 -0.026133    0.99566001 -0.15827\n",
      " -0.26201999  0.17324001  0.060104   -0.48004001  0.23841    -0.21495\n",
      "  0.077693   -0.089078    0.12985    -0.17399999 -0.057151    0.48207\n",
      " -0.14668     0.26739001 -0.33366001  0.32552001  0.62519997 -0.30904999\n",
      "  0.087737   -0.17204     0.28246    -0.037268    0.16007     0.30030999\n",
      "  1.40610003 -0.32168999 -0.025792    0.037175    0.026222   -0.27671\n",
      "  0.051688   -0.058734   -0.23222999 -0.10529    -0.40318    -0.22160999\n",
      "  0.060587    0.091321   -0.21363001  0.071634   -0.21331     0.074621\n",
      "  0.012001   -0.21952   ]\n"
     ]
    }
   ],
   "source": [
    "print(embedding_matrix[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Embedding, TimeDistributed, Dense, RepeatVector,\\\n",
    "                         Activation, Flatten, Reshape, concatenate, Dropout, BatchNormalization\n",
    "#from keras.optimizers import Adam, RMSprop\n",
    "#from keras.layers.wrappers import Bidirectional\n",
    "from keras.layers import add\n",
    "#from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras import Input, layers\n",
    "from keras import optimizers\n",
    "#from keras.applications.inception_v3 import preprocess_input\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(descriptions, photos, wordtoix, max_length):\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    for key, desc_list in descriptions.items():\n",
    "        photo = photos[key+'.jpg']\n",
    "        for desc in desc_list:\n",
    "                # encode the sequence\n",
    "            seq = [wordtoix[word] for word in desc.split(' ') if word in wordtoix]\n",
    "                # split one sequence into multiple X, y pairs\n",
    "            for i in range(1, len(seq)):\n",
    "                    # split into input and output pair\n",
    "                in_seq, out_seq = seq[:i], seq[i]\n",
    "                    # pad input sequence\n",
    "                in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "                    # encode output sequence\n",
    "                out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "                    # store\n",
    "                X1.append(photo)\n",
    "                X2.append(in_seq)\n",
    "                y.append(out_seq)\n",
    "            # yield the batch data        \n",
    "    return [[np.array(X1), np.array(X2)], np.array(y)]\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "startseq child in pink dress is climbing up set of stairs in an entry way endseq\n",
      "startseq girl going into wooden building endseq\n",
      "startseq little girl climbing into wooden playhouse endseq\n",
      "startseq little girl climbing the stairs to her playhouse endseq\n",
      "startseq little girl in pink dress going into wooden cabin endseq\n"
     ]
    }
   ],
   "source": [
    "item=train_descriptions['1000268201_693b08cb0e']\n",
    "x,y=list(),list()\n",
    "m,n=list(),list()\n",
    "for desc in item:\n",
    "    print(desc)\n",
    "    seq = [wordtoix[word] for word in desc.split(' ') if word in wordtoix]\n",
    "    for i in range(1, len(seq)):      \n",
    "                in_seq, out_seq = seq[:i], seq[i]\n",
    "                    # pad input sequence\n",
    "                m.append(in_seq)\n",
    "                n.append(out_seq)\n",
    "                in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "                    # encode output sequence\n",
    "                out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "                    # store\n",
    "                #X1.append(photo)\n",
    "                x.append(in_seq)\n",
    "                y.append(out_seq)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n"
     ]
    }
   ],
   "source": [
    "print(len(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n"
     ]
    }
   ],
   "source": [
    "print(len(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0., 0., 1., ..., 0., 0., 0.], dtype=float32), array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Photos: train=6000\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "train_features = pickle.load(open(\"encoded_train_images.pkl\", \"rb\"))\n",
    "print('Photos: train=%d' % len(train_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#epochs = 10\n",
    "number_pics_per_bath = 6000\n",
    "steps = len(train_descriptions)//number_pics_per_bath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = data_generator(train_descriptions, train_features, wordtoix, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "292328\n"
     ]
    }
   ],
   "source": [
    "print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs1 = Input(shape=(4096,))\n",
    "fe1 = Dropout(0.5)(inputs1)\n",
    "fe2 = Dense(256, activation='relu')(fe1)\n",
    "inputs2 = Input(shape=(max_length,))\n",
    "se1 = Embedding(vocab_size, embedding_dim, mask_zero=True)(inputs2)\n",
    "se2 = Dropout(0.5)(se1)\n",
    "se3 = LSTM(256)(se2)\n",
    "decoder1 = add([fe2, se3])\n",
    "decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "model = Model(inputs=[inputs1, inputs2], outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 34)]         0           []                               \n",
      "                                                                                                  \n",
      " input_1 (InputLayer)           [(None, 4096)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 34, 200)      330400      ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 4096)         0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 34, 200)      0           ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 256)          1048832     ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    (None, 256)          467968      ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 256)          0           ['dense[0][0]',                  \n",
      "                                                                  'lstm[0][0]']                   \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 256)          65792       ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 1652)         424564      ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,337,556\n",
      "Trainable params: 2,337,556\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[2].set_weights([embedding_matrix])\n",
    "model.layers[2].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "9136/9136 [==============================] - 486s 53ms/step - loss: 4.0190\n",
      "Epoch 2/10\n",
      "9136/9136 [==============================] - 504s 55ms/step - loss: 3.4733\n",
      "Epoch 3/10\n",
      "5547/9136 [=================>............] - ETA: 3:16 - loss: 3.3392"
     ]
    }
   ],
   "source": [
    "#model.optimizer.lr = 0.0001\n",
    "model.fit(X,y, epochs=10, verbose=1)\n",
    "#model.save('./model_weights/model_' + str(i) + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('First_try.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"encoded_test_images.pkl\", \"rb\") as encoded_pickle:\n",
    "    encoding_test = pickle.load(encoded_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedySearch(photo):\n",
    "    in_text = 'startseq'\n",
    "    for i in range(max_length):\n",
    "        sequence = [wordtoix[w] for w in in_text.split() if w in wordtoix]\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        yhat = model.predict([photo,sequence], verbose=0)\n",
    "        yhat = np.argmax(yhat)\n",
    "        word = ixtoword[yhat]\n",
    "        in_text += ' ' + word\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    final = in_text.split()\n",
    "    final = final[1:-1]\n",
    "    final = ' '.join(final)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = 'Flicker8k_Dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#z=0\n",
    "z+=1\n",
    "pic = list(encoding_test.keys())[z]\n",
    "image = encoding_test[pic].reshape((1,4096))\n",
    "x=plt.imread(images+pic)\n",
    "plt.imshow(x)\n",
    "plt.show()\n",
    "print(\"Greedy:\",greedySearch(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
