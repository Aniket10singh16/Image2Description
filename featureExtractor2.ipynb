{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import string\n",
    "import os\n",
    "from PIL import Image\n",
    "import glob\n",
    "from time import time\n",
    "from ultralytics import YOLO\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.models import Model\n",
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#from dgl.nn.pytorch.conv import RelGraphConv\n",
    "from dgl.nn import GraphConv\n",
    "#import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import cv2\n",
    "from tensorflow.keras.preprocessing.image import load_img,img_to_array\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "\n",
    "global_resize_image=(512,512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 14575\n"
     ]
    }
   ],
   "source": [
    "def load_set(filename):\n",
    "\tdoc = load_doc(filename)\n",
    "\tdataset = list()\n",
    "\t# process line by line\n",
    "\tfor line in doc.split('\\n'):\n",
    "\t\t# skip empty lines\n",
    "\t\tif len(line) < 1:\n",
    "\t\t\tcontinue\n",
    "\t\t# get the image identifier\n",
    "\t\tidentifier = line.split('.')[0]\n",
    "\t\tdataset.append(identifier)\n",
    "\treturn set(dataset)\n",
    "\n",
    "# load training dataset (6K)\n",
    "#filename = 'Flickr_1k.trainImages.txt'\n",
    "filename='D:\\\\Paper\\\\Mimic Human Level Intelligence in Image Descriptioning\\\\train_split.txt'\n",
    "train = load_set(filename)\n",
    "print('Dataset: %d' % len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = 'D:/Paper/Mimic Human Level Intelligence in Image Descriptioning/visual_genome/images/VG_100K/'\n",
    "\n",
    "# Create a list of all image names in the directory\n",
    "img = glob.glob(images + '*.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:/Paper/Mimic Human Level Intelligence in Image Descriptioning/visual_genome/images/VG_100K/\n"
     ]
    }
   ],
   "source": [
    "print(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_file = 'D:\\\\Paper\\\\Mimic Human Level Intelligence in Image Descriptioning\\\\train_split.txt'\n",
    "# Read the train image names in a set\n",
    "train_images = set(open(train_images_file, 'r').read().strip().split('\\n'))\n",
    "\n",
    "# Create a list of all the training images with their full path names\n",
    "train_img = []\n",
    "\n",
    "for i in img: # img is list of full path names of all images\n",
    "    if i[len(images):] in train_images: # Check if the image belongs to training set\n",
    "        train_img.append(i) # Add it to the list of train images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below file conatains the names of images to be used in test data\n",
    "test_images_file = 'D:\\\\Paper\\\\Mimic Human Level Intelligence in Image Descriptioning\\\\test_split.txt'\n",
    "# Read the validation image names in a set# Read the test image names in a set\n",
    "test_images = set(open(test_images_file, 'r').read().strip().split('\\n'))\n",
    "\n",
    "# Create a list of all the test images with their full path names\n",
    "test_img = []\n",
    "\n",
    "for i in img: # img is list of full path names of all images\n",
    "    if i[len(images):] in test_images: # Check if the image belongs to test set\n",
    "        test_img.append(i) # Add it to the list of test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2489"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image_path):\n",
    "    \n",
    "    img = image.load_img(image_path, target_size=(299, 299))\n",
    "    # Convert PIL image to numpy array of 3-dimensions\n",
    "    x = image.img_to_array(img)\n",
    "    # Add one more dimension\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    # preprocess the images using preprocess_input() from inception module\n",
    "    x = preprocess_input(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels.h5\n",
      "96116736/96112376 [==============================] - 10s 0us/step\n",
      "96124928/96112376 [==============================] - 10s 0us/step\n"
     ]
    }
   ],
   "source": [
    "model = InceptionV3(weights='imagenet')\n",
    "model_new = Model(model.input, model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "def Convert_to_DataFrame(results):\n",
    "    boxes = results[0].boxes.xyxy.tolist()\n",
    "    classes = results[0].boxes.cls.tolist()\n",
    "    names = results[0].names\n",
    "    confidences = results[0].boxes.conf.tolist()\n",
    "    data = {'xmin': [box[0] for box in boxes],\n",
    "        'ymin': [box[1] for box in boxes],\n",
    "        'xmax': [box[2] for box in boxes],\n",
    "        'ymax': [box[3] for box in boxes],\n",
    "        'confidence': confidences,\n",
    "        'class': classes,\n",
    "        'name': [names[int(cls)] for cls in classes]}\n",
    "    output_df = pd.DataFrame(data)\n",
    "    #print(output_df)\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Object Detection using YOLOv8\n",
    "def detect_objects(imagePath):\n",
    "    image=cv2.imread(imagePath)\n",
    "    image=cv2.resize(image,global_resize_image)\n",
    "    model = YOLO('yolov8x.pt')\n",
    "    results=model(image)\n",
    "    output_df=Convert_to_DataFrame(results)\n",
    "    return output_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_nodes(image):\n",
    "    output_df=detect_objects(image)\n",
    "    image_nodes=[]\n",
    "    for i in range(output_df.shape[0]):\n",
    "        #print(output_df.iloc[i]['name'])\n",
    "        data={\"object_id\":i,\n",
    "        \"start_point\":(round(output_df.iloc[i]['xmin']),round(output_df.iloc[i]['ymin'])),\n",
    "        \"ending_point\":(round(output_df.iloc[i]['xmax']),round(output_df.iloc[i]['ymax'])),\n",
    "        \"label\":output_df.iloc[i]['name']\n",
    "        }\n",
    "        image_nodes.append(data)\n",
    "    return image_nodes  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_nodeImages(image,nodes):\n",
    "    image=cv2.resize(image,(256,256))\n",
    "    #image = cv2.rectangle(image, (106,19), (188,133), (255, 0, 0) , 2) \n",
    "    #crop=image[19:133,106:188]\n",
    "    #cv2.imshow('image',crop)\n",
    "    #cv2.waitKey()\n",
    "    segment_array=[]\n",
    "    for i in range(len(nodes)):\n",
    "        start=nodes[i].get('start_point')\n",
    "        end=nodes[i].get('ending_point')\n",
    "        #print(end[1])\n",
    "        a,b,c,d=start[1],end[1],start[0],end[0]\n",
    "        crop=image[a:b,c:d]\n",
    "        segment_array.append(crop)\n",
    "    segment_array.append(image)    \n",
    "    return segment_array    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featuresVGG(segment_array):\n",
    "    VGG_features=[]\n",
    "    for image in segment_array:\n",
    "        image=cv2.resize(image,(299,299))\n",
    "        image=img_to_array(image)\n",
    "        image=image.reshape((1,image.shape[0],image.shape[1],image.shape[2]))\n",
    "        image=preprocess_input(image)\n",
    "        feature=model.predict(image)\n",
    "        VGG_features.append(feature)\n",
    "    return VGG_features    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "image=cv2.imread(r\"D:\\\\Paper\\\\Mimic Human Level Intelligence in Image Descriptioning\\\\Flicker8k_Dataset\\\\109202801_c6381eef15.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Riddhick/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2024-2-22 Python-3.9.5 torch-2.2.0+cpu CPU\n",
      "\n",
      "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt to yolov5s.pt...\n",
      "100%|██████████| 14.1M/14.1M [00:00<00:00, 29.9MB/s]\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "nodes=extract_nodes(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_array=generate_nodeImages(image,nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG=featuresVGG(segment_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(segment_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1000)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VGG[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_features(feature_list):\n",
    "    \n",
    "    g = dgl.DGLGraph()\n",
    "    num_nodes = len(feature_list)\n",
    "    g.add_nodes(num_nodes)\n",
    "    features = torch.tensor(feature_list, dtype=torch.float32)\n",
    "    g.ndata['features'] = features\n",
    "    num_edges = num_nodes*num_nodes-1 \n",
    "    g.add_edges(torch.randint(num_nodes, (num_edges,)), torch.randint(num_nodes, (num_edges,)))\n",
    "    g.edata['edge_weights'] = nn.Parameter(torch.rand(num_edges, requires_grad=True))\n",
    "\n",
    "# Define a Graph Convolutional Network (GCN) model\n",
    "    class GCN(nn.Module):\n",
    "        def __init__(self, in_feats, hidden_feats):\n",
    "            super(GCN, self).__init__()\n",
    "            self.conv = GraphConv(in_feats, hidden_feats)\n",
    "\n",
    "        def forward(self, g, features):\n",
    "            # Perform graph convolution with learnable edge weights\n",
    "            h = self.conv(g, features)\n",
    "            h = F.relu(h)\n",
    "            return h\n",
    "\n",
    "# Reshape the features tensor\n",
    "    features = features.squeeze(1)\n",
    "\n",
    "    # Instantiate the GCN model\n",
    "    in_feats = features.shape[1] # Number of input features\n",
    "    hidden_feats = 256 # Number of hidden units\n",
    "    model = GCN(in_feats, hidden_feats)\n",
    "\n",
    "    # Forward pass\n",
    "    output_gcn = model(g, features)\n",
    "    output=output_gcn.detach().numpy()\n",
    "    output=output.transpose()\n",
    "    #b=output.reshape(1,-1)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=graph_features(VGG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 4)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_reduction(features):\n",
    "    pca = PCA(n_components=1)\n",
    "    pca.fit(features)\n",
    "    s=StandardScaler()\n",
    "    x=pca.transform(features)\n",
    "    x=np.reshape(x,x.shape[0])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=feature_reduction(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256,)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_extraction(image_path):\n",
    "    img= cv2.imread(image_path)\n",
    "    nodes=extract_nodes(img)\n",
    "    segment_array=generate_nodeImages(img,nodes)\n",
    "    feature_vectors=featuresVGG(segment_array)\n",
    "    print(len(nodes))\n",
    "    gcn_features=graph_features(feature_vectors)\n",
    "    feature=feature_reduction(gcn_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Riddhick/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2024-2-22 Python-3.9.5 torch-2.2.0+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "m=complete_extraction(\"D:\\\\Paper\\\\Mimic Human Level Intelligence in Image Descriptioning\\\\visual_genome\\\\images\\\\VG_100K\\\\112.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:/Paper/Mimic Human Level Intelligence in Image Descriptioning/visual_genome/images/VG_100K\\1001.jpg\n",
      "D:/Paper/Mimic Human Level Intelligence in Image Descriptioning/visual_genome/images/VG_100K\\1025.jpg\n",
      "D:/Paper/Mimic Human Level Intelligence in Image Descriptioning/visual_genome/images/VG_100K\\1060.jpg\n",
      "D:/Paper/Mimic Human Level Intelligence in Image Descriptioning/visual_genome/images/VG_100K\\1068.jpg\n",
      "D:/Paper/Mimic Human Level Intelligence in Image Descriptioning/visual_genome/images/VG_100K\\108.jpg\n",
      "D:/Paper/Mimic Human Level Intelligence in Image Descriptioning/visual_genome/images/VG_100K\\1081.jpg\n",
      "D:/Paper/Mimic Human Level Intelligence in Image Descriptioning/visual_genome/images/VG_100K\\1082.jpg\n",
      "D:/Paper/Mimic Human Level Intelligence in Image Descriptioning/visual_genome/images/VG_100K\\1090.jpg\n",
      "D:/Paper/Mimic Human Level Intelligence in Image Descriptioning/visual_genome/images/VG_100K\\1122.jpg\n",
      "D:/Paper/Mimic Human Level Intelligence in Image Descriptioning/visual_genome/images/VG_100K\\1159.jpg\n",
      "Time taken in seconds = 0.001996278762817383\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "encoding_train = {}\n",
    "for i in range(10):\n",
    "    print(train_img[i])\n",
    "print(\"Time taken in seconds =\", time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
