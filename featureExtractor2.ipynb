{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import string\n",
    "import os\n",
    "from PIL import Image\n",
    "import glob\n",
    "from time import time\n",
    "from ultralytics import YOLO\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "#from dgl.nn.pytorch.conv import RelGraphConv\n",
    "from dgl.nn import GraphConv\n",
    "#import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import cv2\n",
    "from tensorflow.keras.preprocessing.image import load_img,img_to_array\n",
    "from tensorflow.keras.applications.vgg19 import VGG19, preprocess_input\n",
    "\n",
    "global_resize_image=(512,512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 14575\n"
     ]
    }
   ],
   "source": [
    "def load_set(filename):\n",
    "\tdoc = load_doc(filename)\n",
    "\tdataset = list()\n",
    "\t# process line by line\n",
    "\tfor line in doc.split('\\n'):\n",
    "\t\t# skip empty lines\n",
    "\t\tif len(line) < 1:\n",
    "\t\t\tcontinue\n",
    "\t\t# get the image identifier\n",
    "\t\tidentifier = line.split('.')[0]\n",
    "\t\tdataset.append(identifier)\n",
    "\treturn set(dataset)\n",
    "\n",
    "# load training dataset (6K)\n",
    "#filename = 'Flickr_1k.trainImages.txt'\n",
    "filename='train_split.txt'\n",
    "train = load_set(filename)\n",
    "print('Dataset: %d' % len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = 'VG_100K/'\n",
    "\n",
    "# Create a list of all image names in the directory\n",
    "img = glob.glob(images + '*.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VG_100K/\n"
     ]
    }
   ],
   "source": [
    "print(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_file = 'train_split.txt'\n",
    "# Read the train image names in a set\n",
    "train_images = set(open(train_images_file, 'r').read().strip().split('\\n'))\n",
    "\n",
    "# Create a list of all the training images with their full path names\n",
    "train_img = []\n",
    "\n",
    "for i in img: # img is list of full path names of all images\n",
    "    if i[len(images):] in train_images: # Check if the image belongs to training set\n",
    "        train_img.append(i) # Add it to the list of train images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'test_split.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m test_images_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_split.txt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Read the validation image names in a set# Read the test image names in a set\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m test_images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtest_images_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Create a list of all the test images with their full path names\u001b[39;00m\n\u001b[0;32m      7\u001b[0m test_img \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'test_split.txt'"
     ]
    }
   ],
   "source": [
    "# Below file conatains the names of images to be used in test data\n",
    "test_images_file = 'test_split.txt'\n",
    "# Read the validation image names in a set# Read the test image names in a set\n",
    "test_images = set(open(test_images_file, 'r').read().strip().split('\\n'))\n",
    "\n",
    "# Create a list of all the test images with their full path names\n",
    "test_img = []\n",
    "\n",
    "for i in img: # img is list of full path names of all images\n",
    "    if i[len(images):] in test_images: # Check if the image belongs to test set\n",
    "        test_img.append(i) # Add it to the list of test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2489"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image_path):\n",
    "    \n",
    "    img = image.load_img(image_path, target_size=(224, 224))\n",
    "    # Convert PIL image to numpy array of 3-dimensions\n",
    "    x = image.img_to_array(img)\n",
    "    # Add one more dimension\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    # preprocess the images using preprocess_input() from inception module\n",
    "    x = preprocess_input(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\EXTREME_RED\\anaconda3\\envs\\Final_Project_V2\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\EXTREME_RED\\anaconda3\\envs\\Final_Project_V2\\Lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv4 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv4 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv4 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 4096)              102764544 \n",
      "                                                                 \n",
      " fc2 (Dense)                 (None, 4096)              16781312  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 139570240 (532.42 MB)\n",
      "Trainable params: 139570240 (532.42 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model=VGG19()\n",
    "model=Model(inputs=model.inputs,outputs=model.layers[-2].output)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "def Convert_to_DataFrame(results):\n",
    "    boxes = results[0].boxes.xyxy.tolist()\n",
    "    classes = results[0].boxes.cls.tolist()\n",
    "    names = results[0].names\n",
    "    confidences = results[0].boxes.conf.tolist()\n",
    "    data = {'xmin': [box[0] for box in boxes],\n",
    "        'ymin': [box[1] for box in boxes],\n",
    "        'xmax': [box[2] for box in boxes],\n",
    "        'ymax': [box[3] for box in boxes],\n",
    "        'confidence': confidences,\n",
    "        'class': classes,\n",
    "        'name': [names[int(cls)] for cls in classes]}\n",
    "    output_df = pd.DataFrame(data)\n",
    "    #print(output_df)\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Object Detection using YOLOv8\n",
    "def detect_objects(imagePath):\n",
    "    image=cv2.imread(imagePath)\n",
    "    image=cv2.resize(image,global_resize_image)\n",
    "    model = YOLO('yolov8x.pt')\n",
    "    results=model(image)\n",
    "    #results=Convert_to_DataFrame(results)\n",
    "    return results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results=detect_objects(imagePath)\n",
    "#print(len(results[0].boxes))\n",
    "#print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results\n",
    "for i, r in enumerate(results):\n",
    "    # Plot results image\n",
    "    im_bgr = r.plot()  # BGR-order numpy array\n",
    "    #im_array = im_bgr.numpy()\n",
    "    im_rgb = Image.fromarray(im_bgr[..., ::-1])  # RGB-order PIL image\n",
    "\n",
    "    # Show results to screen (in supported environments)\n",
    "    #r.show()\n",
    "    plt.imshow(numpy.array(im_rgb))\n",
    "    plt.show()\n",
    "\n",
    "    # Save results to disk\n",
    "    #r.save(filename=f'results{i}.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<==================  Region Proposal ======================>\n",
    "\n",
    "\n",
    "def roi_propose(boxes):\n",
    "    # Function to calculate IoU\n",
    "    def calculate_iou(box1, box2):\n",
    "        # Calculate intersection rectangle coordinates\n",
    "        x1 = max(box1[0], box2[0])\n",
    "        y1 = max(box1[1], box2[1])\n",
    "        x2 = min(box1[2], box2[2])\n",
    "        y2 = min(box1[3], box2[3])\n",
    "        \n",
    "        # Calculate area of intersection rectangle\n",
    "        intersection_area = max(0, x2 - x1 + 1) * max(0, y2 - y1 + 1)\n",
    "        \n",
    "        # Calculate area of both bounding boxes\n",
    "        box1_area = (box1[2] - box1[0] + 1) * (box1[3] - box1[1] + 1)\n",
    "        box2_area = (box2[2] - box2[0] + 1) * (box2[3] - box2[1] + 1)\n",
    "        \n",
    "        # Calculate IoU\n",
    "        iou = intersection_area / float(box1_area + box2_area - intersection_area)\n",
    "        \n",
    "        return iou\n",
    "\n",
    "    # Calculate IoU matrix\n",
    "    num_boxes = len(boxes)\n",
    "    iou_matrix = np.zeros((num_boxes, num_boxes))\n",
    "    for i in range(num_boxes):\n",
    "        for j in range(i + 1, num_boxes):\n",
    "            iou_matrix[i, j] = calculate_iou(boxes[i], boxes[j])\n",
    "            iou_matrix[j, i] = iou_matrix[i, j]  # IoU is symmetric\n",
    "\n",
    "    # Define threshold for IoU\n",
    "    iou_threshold = 0.5\n",
    "\n",
    "    # Merge bounding boxes based on IoU\n",
    "    merged = set()\n",
    "    groups = []\n",
    "    for i in range(num_boxes):\n",
    "        if i not in merged:\n",
    "            group = [i]\n",
    "            for j in range(i + 1, num_boxes):\n",
    "                if j not in merged and iou_matrix[i, j] >= iou_threshold:\n",
    "                    group.append(j)\n",
    "                    merged.add(j)\n",
    "            groups.append(group)\n",
    "    # Merge regions completely inside others\n",
    "    for i, box1 in enumerate(boxes):\n",
    "        for j, box2 in enumerate(boxes):\n",
    "            if i != j and calculate_iou(box1, box2) == 1:\n",
    "                if i in merged:\n",
    "                    groups.remove(groups[i])\n",
    "                elif j in merged:\n",
    "                    groups.remove(groups[j])\n",
    "    # Create ROIs based on merged bounding boxes\n",
    "    rois = {}\n",
    "    for idx, group in enumerate(groups):\n",
    "        roi_boxes = [boxes[i] for i in group]\n",
    "        min_x = min(box[0] for box in roi_boxes)\n",
    "        min_y = min(box[1] for box in roi_boxes)\n",
    "        max_x = max(box[2] for box in roi_boxes)\n",
    "        max_y = max(box[3] for box in roi_boxes)\n",
    "        rois[idx] = (min_x, min_y, max_x, max_y)\n",
    "\n",
    "    # Function to check if two ROIs overlap\n",
    "    def is_inside(roi1, roi2):\n",
    "        x1_min, y1_min, x1_max, y1_max = roi1\n",
    "        x2_min, y2_min, x2_max, y2_max = roi2\n",
    "        return (x1_min >= x2_min and y1_min >= y2_min and\n",
    "                x1_max <= x2_max and y1_max <= y2_max)\n",
    "\n",
    "    # Function to merge ROIs if one ROI completely encompasses the other\n",
    "    def merge_overlapping_rois(rois):\n",
    "        merged_rois = {}\n",
    "        for idx, roi in rois.items():\n",
    "            merge = True\n",
    "            for merged_idx, merged_roi in merged_rois.items():\n",
    "                if is_inside(roi, merged_roi):\n",
    "                    # ROI is completely inside another ROI, don't merge\n",
    "                    merge = False\n",
    "                    break\n",
    "                elif is_inside(merged_roi, roi):\n",
    "                    # Another ROI is completely inside this ROI, merge\n",
    "                    merged_rois[merged_idx] = roi\n",
    "                    merge = False\n",
    "                    break\n",
    "            if merge:\n",
    "                merged_rois[idx] = roi\n",
    "        return merged_rois\n",
    "    # Merge overlapping ROIs\n",
    "    merged_rois = merge_overlapping_rois(rois)\n",
    "\n",
    "    # Print merged ROIs\n",
    "    for idx, roi in enumerate(merged_rois):\n",
    "        print(f\"ROI {idx}: {roi}\")\n",
    "    '''    \n",
    "    # Print proposed ROIs\n",
    "    for roi_id, roi_coords in rois.items():\n",
    "        print(f\"ROI {roi_id}: {roi_coords}\")'''\n",
    "    return merged_rois    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_nodes(image):\n",
    "    results=detect_objects(image)\n",
    "    proposed_roi = roi_propose(results[0].boxes.xyxy.tolist()) #Roi calculation\n",
    "    output_df=Convert_to_DataFrame(results)\n",
    "    image_nodes=[]\n",
    "    for i in range(output_df.shape[0]):\n",
    "        #print(output_df.iloc[i]['name'])\n",
    "        data={\"object_id\":i,\n",
    "        \"start_point\":(round(output_df.iloc[i]['xmin']),round(output_df.iloc[i]['ymin'])),\n",
    "        \"ending_point\":(round(output_df.iloc[i]['xmax']),round(output_df.iloc[i]['ymax'])),\n",
    "        \"label\":output_df.iloc[i]['name']\n",
    "        }\n",
    "        image_nodes.append(data)\n",
    "    return image_nodes    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<====================  Visualize and save the roi drawn images ===============>\n",
    "\n",
    "'''import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# Function to visualize ROIs on the image\n",
    "def visualize_rois(image_path, rois, save_path):\n",
    "    # Read the image\n",
    "    image=cv2.imread(imagePath)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image=cv2.resize(image,global_resize_image)\n",
    "\n",
    "    # Create figure and axes\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Display the image\n",
    "    ax.imshow(image)\n",
    "\n",
    "    # Draw ROIs on the image\n",
    "    for roi_id, roi_coords in rois.items():\n",
    "        min_x, min_y, max_x, max_y = roi_coords\n",
    "        width = max_x - min_x\n",
    "        height = max_y - min_y\n",
    "        rect = patches.Rectangle((min_x, min_y), width, height, linewidth=2, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "        # Add text label\n",
    "        ax.text(min_x, min_y - 5, f'ROI {roi_id}', color='r')\n",
    "\n",
    "    # Remove axis ticks\n",
    "    ax.axis('off')\n",
    "\n",
    "    # Save the image\n",
    "    plt.savefig(save_path, bbox_inches='tight', pad_inches=0)\n",
    "\n",
    "    # Show the image\n",
    "    plt.show()\n",
    "\n",
    "# Define image path and save path\n",
    "#image_path = 'your_image.jpg'\n",
    "save_path = 'image_with_rois.jpg'\n",
    "\n",
    "# Visualize ROIs on the image and save\n",
    "visualize_rois(imagePath, roi_propose(results[0].boxes.xyxy.tolist()), save_path)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_nodeImages(imagePath,nodes):\n",
    "    image=cv2.imread(imagePath)\n",
    "    image=cv2.resize(image,(256,256))\n",
    "    #image = cv2.rectangle(image, (106,19), (188,133), (255, 0, 0) , 2) \n",
    "    #crop=image[19:133,106:188]\n",
    "    #cv2.imshow('image',crop)\n",
    "    #cv2.waitKey()\n",
    "    segment_array=[]\n",
    "    for i in range(len(nodes)):\n",
    "        start=nodes[i].get('start_point')\n",
    "        end=nodes[i].get('ending_point')\n",
    "        #print(end[1])\n",
    "        a,b,c,d=start[1],end[1],start[0],end[0]\n",
    "        crop=image[a:b,c:d]\n",
    "        segment_array.append(crop)\n",
    "    segment_array.append(image)    \n",
    "    return segment_array    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featuresVGG(segment_array):\n",
    "    VGG_features=[]\n",
    "    for image in segment_array:\n",
    "        image=cv2.resize(image,(224,224))\n",
    "        image=img_to_array(image)\n",
    "        image=image.reshape((1,image.shape[0],image.shape[1],image.shape[2]))\n",
    "        image=preprocess_input(image)\n",
    "        feature=model.predict(image)\n",
    "        VGG_features.append(feature)\n",
    "    return VGG_features    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "image=cv2.imread(r\"D:\\\\Paper\\\\Mimic Human Level Intelligence in Image Descriptioning\\\\Flicker8k_Dataset\\\\109202801_c6381eef15.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Riddhick/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2024-2-22 Python-3.9.5 torch-2.2.0+cpu CPU\n",
      "\n",
      "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt to yolov5s.pt...\n",
      "100%|██████████| 14.1M/14.1M [00:00<00:00, 29.9MB/s]\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "nodes=extract_nodes(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_array=generate_nodeImages(image,nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG=featuresVGG(segment_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(segment_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1000)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VGG[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_features(feature_list):\n",
    "    \n",
    "    g = dgl.DGLGraph()\n",
    "    num_nodes = len(feature_list)\n",
    "    g.add_nodes(num_nodes)\n",
    "    features = torch.tensor(feature_list, dtype=torch.float32)\n",
    "    g.ndata['features'] = features\n",
    "    num_edges = num_nodes*num_nodes-1 \n",
    "    g.add_edges(torch.randint(num_nodes, (num_edges,)), torch.randint(num_nodes, (num_edges,)))\n",
    "    g.edata['edge_weights'] = nn.Parameter(torch.rand(num_edges, requires_grad=True))\n",
    "\n",
    "# Define a Graph Convolutional Network (GCN) model\n",
    "    class GCN(nn.Module):\n",
    "        def __init__(self, in_feats, hidden_feats):\n",
    "            super(GCN, self).__init__()\n",
    "            self.conv = GraphConv(in_feats, hidden_feats)\n",
    "\n",
    "        def forward(self, g, features):\n",
    "            # Perform graph convolution with learnable edge weights\n",
    "            h = self.conv(g, features)\n",
    "            h = F.relu(h)\n",
    "            return h\n",
    "\n",
    "# Reshape the features tensor\n",
    "    features = features.squeeze(1)\n",
    "\n",
    "    # Instantiate the GCN model\n",
    "    in_feats = features.shape[1] # Number of input features\n",
    "    hidden_feats = 256 # Number of hidden units\n",
    "    model = GCN(in_feats, hidden_feats)\n",
    "\n",
    "    # Forward pass\n",
    "    output_gcn = model(g, features)\n",
    "    output=output_gcn.detach().numpy()\n",
    "    output=output.transpose()\n",
    "    #b=output.reshape(1,-1)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=graph_features(VGG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 4)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_reduction(features):\n",
    "    pca = PCA(n_components=1)\n",
    "    pca.fit(features)\n",
    "    s=StandardScaler()\n",
    "    x=pca.transform(features)\n",
    "    x=np.reshape(x,x.shape[0])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=feature_reduction(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256,)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_extraction(image_path):\n",
    "    img= cv2.imread(image_path)\n",
    "    nodes=extract_nodes(img)\n",
    "    segment_array=generate_nodeImages(img,nodes)\n",
    "    feature_vectors=featuresVGG(segment_array)\n",
    "    print(len(nodes))\n",
    "    gcn_features=graph_features(feature_vectors)\n",
    "    feature=feature_reduction(gcn_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Riddhick/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2024-2-22 Python-3.9.5 torch-2.2.0+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "m=complete_extraction(\"D:\\\\Paper\\\\Mimic Human Level Intelligence in Image Descriptioning\\\\visual_genome\\\\images\\\\VG_100K\\\\112.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:/Paper/Mimic Human Level Intelligence in Image Descriptioning/visual_genome/images/VG_100K\\1001.jpg\n",
      "D:/Paper/Mimic Human Level Intelligence in Image Descriptioning/visual_genome/images/VG_100K\\1025.jpg\n",
      "D:/Paper/Mimic Human Level Intelligence in Image Descriptioning/visual_genome/images/VG_100K\\1060.jpg\n",
      "D:/Paper/Mimic Human Level Intelligence in Image Descriptioning/visual_genome/images/VG_100K\\1068.jpg\n",
      "D:/Paper/Mimic Human Level Intelligence in Image Descriptioning/visual_genome/images/VG_100K\\108.jpg\n",
      "D:/Paper/Mimic Human Level Intelligence in Image Descriptioning/visual_genome/images/VG_100K\\1081.jpg\n",
      "D:/Paper/Mimic Human Level Intelligence in Image Descriptioning/visual_genome/images/VG_100K\\1082.jpg\n",
      "D:/Paper/Mimic Human Level Intelligence in Image Descriptioning/visual_genome/images/VG_100K\\1090.jpg\n",
      "D:/Paper/Mimic Human Level Intelligence in Image Descriptioning/visual_genome/images/VG_100K\\1122.jpg\n",
      "D:/Paper/Mimic Human Level Intelligence in Image Descriptioning/visual_genome/images/VG_100K\\1159.jpg\n",
      "Time taken in seconds = 0.001996278762817383\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "encoding_train = {}\n",
    "for i in range(10):\n",
    "    print(train_img[i])\n",
    "print(\"Time taken in seconds =\", time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
