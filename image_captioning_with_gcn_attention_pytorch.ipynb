{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s8kuhc8_s4kC",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "7f7b4356-3d15-4a6b-99de-943d74b2e9aa"
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.init as init\n",
    "\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "import torchvision.transforms.functional as f\n",
    "import torchvision.models as models\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "from dgl.nn import GraphConv\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "RyABT53ws4kH",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#location of the data\n",
    "data_location =  \"D:\\\\WORK\\\\M.SC\\\\MSC_Project\\\\GitHub\\\\Dataset\\\\Flicker8k_Dataset\\\\\"\n",
    "#!ls $data_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_v_rOJTLs4kL",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#reading the text data\n",
    "caption_file = \"D:\\\\WORK\\\\M.SC\\\\MSC_Project\\\\GitHub\\\\Image2Description\\\\train_captions.txt\"\n",
    "df = pd.read_csv(caption_file)\n",
    "print(\"There are {} image to captions\".format(len(df)))\n",
    "df.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "DZ2hbm6Ks4kO",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#using spacy for the better text tokenization\n",
    "spacy_eng = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "1S_EvzXPs4kQ",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self,freq_threshold):\n",
    "\n",
    "        self.itos = {0:\"<PAD>\",1:\"<SOS>\",2:\"<EOS>\",3:\"<UNK>\"}\n",
    "\n",
    "        self.stoi = {v:k for k,v in self.itos.items()}\n",
    "\n",
    "        self.freq_threshold = freq_threshold\n",
    "\n",
    "    def __len__(self): return len(self.itos)\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenize(text):\n",
    "        return [token.text.lower() for token in spacy_eng.tokenizer(text)]\n",
    "\n",
    "    def build_vocab(self, sentence_list):\n",
    "        frequencies = Counter()\n",
    "        idx = 4\n",
    "\n",
    "        for sentence in sentence_list:\n",
    "            for word in self.tokenize(sentence):\n",
    "                frequencies[word] += 1\n",
    "\n",
    "                #add the word to the vocab if it reaches minum frequecy threshold\n",
    "                if frequencies[word] == self.freq_threshold:\n",
    "                    self.stoi[word] = idx\n",
    "                    self.itos[idx] = word\n",
    "                    idx += 1\n",
    "\n",
    "    def numericalize(self,text):\n",
    "        \"\"\" For each word in the text corresponding index token for that word form the vocab built as list \"\"\"\n",
    "        tokenized_text = self.tokenize(text)\n",
    "        return [ self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"] for token in tokenized_text ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "rtvdkcx8s4kS",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class ObjectDetectorAndFeatureExtractor(nn.Module):\n",
    "    def __init__(self, pretrained=True, iou_threshold=0.4):\n",
    "        super(ObjectDetectorAndFeatureExtractor, self).__init__()\n",
    "\n",
    "        # Object detection model (Faster R-CNN)\n",
    "        self.object_detection_model = fasterrcnn_resnet50_fpn(pretrained=pretrained)\n",
    "        self.object_detection_model.eval()\n",
    "\n",
    "        # Feature extraction model (VGG19)\n",
    "        vgg = models.vgg19(pretrained=pretrained)\n",
    "        for param in vgg.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.feature_extractor = vgg.features[:36]\n",
    "        self.feature_extractor = nn.Sequential(*self.feature_extractor)\n",
    "        self.iou_threshold = iou_threshold\n",
    "\n",
    "    def forward(self, image):\n",
    "        #graph_list = []\n",
    "        image = image.to(device)\n",
    "        #for image in images:\n",
    "        # Perform object detection\n",
    "        with torch.no_grad():\n",
    "            detections = self.object_detection_model([image])\n",
    "\n",
    "        g = dgl.DGLGraph()\n",
    "        g = g.to(device)\n",
    "\n",
    "        if len(detections[0]['boxes']) == 0:\n",
    "            # No objects detected; use whole image features\n",
    "            image_feature = self.feature_extractor(image.unsqueeze(0))\n",
    "            image_feature = image_feature.view(image_feature.size(0), 49, -1)  # (batch_size, 49, 2048)\n",
    "            g.add_nodes(1, {'features': image_feature})\n",
    "            g.add_edges(0, 0)  # self-loop edge\n",
    "            #graph_list.append(g)\n",
    "            return g\n",
    "\n",
    "        # Add nodes for each detected object and assign features\n",
    "        for box in detections[0]['boxes']:\n",
    "            x1, y1, x2, y2 = map(int, box.tolist())\n",
    "            if x1 >= x2 or y1 >= y2:\n",
    "                # Invalid bounding box, skip it\n",
    "                continue\n",
    "\n",
    "            # Extract object region\n",
    "            object_region = image[:, y1:y2, x1:x2]\n",
    "            if object_region.size(1) == 0 or object_region.size(2) == 0:\n",
    "                # Zero-size crop, skip it\n",
    "                continue\n",
    "\n",
    "            # Resize to a common size\n",
    "            object_region = f.resize(object_region, (224, 224))\n",
    "\n",
    "            # Extract features using VGG19\n",
    "            features = self.feature_extractor(object_region.unsqueeze(0))\n",
    "            features = features.view(features.size(0), 49, -1)  # (batch_size, 49, 2048)\n",
    "            g.add_nodes(1, {'features': features})\n",
    "\n",
    "        num_objects = g.number_of_nodes()\n",
    "\n",
    "        # Add edges based on IoU\n",
    "        for i in range(num_objects):\n",
    "            for j in range(num_objects):\n",
    "                if i != j:\n",
    "                    bbox_i = detections[0]['boxes'][i]\n",
    "                    bbox_j = detections[0]['boxes'][j]\n",
    "                    iou = self.calculate_iou(bbox_i, bbox_j)\n",
    "                    if iou > self.iou_threshold:\n",
    "                        g.add_edges(i, j)\n",
    "\n",
    "        #graph_list.append(g)\n",
    "        #g = dgl.batch(graph_list)\n",
    "        return g\n",
    "\n",
    "    def calculate_iou(self, bbox1, bbox2):\n",
    "        xmin = max(bbox1[0], bbox2[0])\n",
    "        ymin = max(bbox1[1], bbox2[1])\n",
    "        xmax = min(bbox1[2], bbox2[2])\n",
    "        ymax = min(bbox1[3], bbox2[3])\n",
    "\n",
    "        intersection_area = max(0, xmax - xmin) * max(0, ymax - ymin)\n",
    "\n",
    "        area1 = (bbox1[2] - bbox1[0]) * (bbox1[3] - bbox1[1])\n",
    "        area2 = (bbox2[2] - bbox2[0]) * (bbox2[3] - bbox2[1])\n",
    "        union_area = area1 + area2 - intersection_area\n",
    "\n",
    "        if union_area == 0:\n",
    "            return 0\n",
    "\n",
    "        iou = intersection_area / union_area\n",
    "        return iou\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "1j9eQc1ks4kU",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class FlickrDataset(Dataset):\n",
    "    \"\"\"\n",
    "    FlickrDataset\n",
    "    \"\"\"\n",
    "    def __init__(self,root_dir,captions_file, cache_dir,transform=None,freq_threshold=5,):\n",
    "        self.root_dir = root_dir\n",
    "        self.df = pd.read_csv(captions_file)\n",
    "        self.transform = transform\n",
    "        self.cache_dir = cache_dir\n",
    "        #Get image and caption colum from the dataframe\n",
    "        self.imgs = self.df[\"image\"]\n",
    "        self.captions = self.df[\"caption\"]\n",
    "\n",
    "        #Initialize vocabulary and build vocab\n",
    "        self.vocab = Vocabulary(freq_threshold)\n",
    "        self.vocab.build_vocab(self.captions.tolist())\n",
    "\n",
    "        self.graphs = ObjectDetectorAndFeatureExtractor().to(device)\n",
    "\n",
    "        # Create the cache directory if it doesn't exist\n",
    "        os.makedirs(self.cache_dir, exist_ok=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        caption = self.captions[idx]\n",
    "        img_name = self.imgs[idx]\n",
    "        img_location = os.path.join(self.root_dir,img_name)\n",
    "        img = Image.open(img_location).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "            \n",
    "        # Check if the processed data is cached\n",
    "        cache_path = os.path.join(self.cache_dir, f\"{img_name}_{idx}.pkl\")\n",
    "        if os.path.exists(cache_path):\n",
    "            with open(cache_path, 'rb') as f:\n",
    "                caption_vec, graph = pickle.load(f)\n",
    "                #print(cache_path,\"  \",graph)\n",
    "        else: \n",
    "            #numericalize the caption text\n",
    "            caption_vec = []\n",
    "            caption_vec += [self.vocab.stoi[\"<SOS>\"]]\n",
    "            caption_vec += self.vocab.numericalize(caption)\n",
    "            caption_vec += [self.vocab.stoi[\"<EOS>\"]]\n",
    "    \n",
    "            # Convert PIL Image to Tensor\n",
    "            #img_tensor = T.ToTensor()(img).to(self.device)\n",
    "    \n",
    "            # Get the object detection graph\n",
    "            graph = self.graphs(img)\n",
    "\n",
    "            # Cache the processed data\n",
    "            with open(cache_path, 'wb') as f:\n",
    "                pickle.dump((caption_vec, graph), f)\n",
    "\n",
    "        return img, torch.tensor(caption_vec), graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "05Mg2BOHs4kZ",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class CapsCollate:\n",
    "\n",
    "    def __init__(self, pad_idx, batch_first=False):\n",
    "        self.pad_idx = pad_idx\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        # Extract images, captions, and graphs from the batch\n",
    "        imgs = [item[0] for item in batch]\n",
    "        captions = [item[1] for item in batch]\n",
    "        graphs = [item[2] for item in batch]\n",
    "\n",
    "        # Concatenate images into a single tensor\n",
    "        imgs = torch.stack(imgs, dim=0)\n",
    "\n",
    "        # Pad captions to ensure consistent lengths within the batch\n",
    "        padded_captions = pad_sequence(captions, batch_first=self.batch_first, padding_value=self.pad_idx)\n",
    "\n",
    "        # Batch the graphs to handle multiple graphs in a single input\n",
    "        batched_graphs = dgl.batch(graphs)\n",
    "\n",
    "        return imgs, padded_captions, batched_graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "3khnskAis4kb",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Helper function to plot the Tensor image\n",
    "def show_image(img, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "\n",
    "    #unnormalize\n",
    "    img[0] = img[0] * 0.229\n",
    "    img[1] = img[1] * 0.224\n",
    "    img[2] = img[2] * 0.225\n",
    "    img[0] += 0.485\n",
    "    img[1] += 0.456\n",
    "    img[2] += 0.406\n",
    "\n",
    "    img = img.numpy().transpose((1, 2, 0))\n",
    "\n",
    "\n",
    "    plt.imshow(img)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "55VTTlius4kd"
   },
   "outputs": [],
   "source": [
    "class test_FlickrDataset(Dataset):\n",
    "    \"\"\"\n",
    "    FlickrDataset\n",
    "    \"\"\"\n",
    "    def __init__(self,root_dir,captions_file, cache_dir,transform=None,freq_threshold=5,):\n",
    "        self.root_dir = root_dir\n",
    "        self.df = pd.read_csv(captions_file)\n",
    "        self.transform = transform\n",
    "        self.cache_dir = cache_dir\n",
    "        #Get image and caption colum from the dataframe\n",
    "        self.imgs = self.df[\"image\"]\n",
    "        self.captions = self.df[\"caption\"]\n",
    "\n",
    "        #Initialize vocabulary and build vocab\n",
    "        self.vocab = Vocabulary(freq_threshold)\n",
    "        self.vocab.build_vocab(self.captions.tolist())\n",
    "\n",
    "        self.graphs = ObjectDetectorAndFeatureExtractor().to(device)\n",
    "\n",
    "        # Create the cache directory if it doesn't exist\n",
    "        os.makedirs(self.cache_dir, exist_ok=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        caption = self.captions[idx]\n",
    "        img_name = self.imgs[idx]\n",
    "        img_location = os.path.join(self.root_dir,img_name)\n",
    "        img = Image.open(img_location).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "            \n",
    "        # Check if the processed data is cached\n",
    "        cache_path = os.path.join(self.cache_dir, f\"{img_name}_{idx}.pkl\")\n",
    "        if os.path.exists(cache_path):\n",
    "            with open(cache_path, 'rb') as f:\n",
    "                graph = pickle.load(f)\n",
    "                #print(cache_path,\"  \",graph)\n",
    "        else: \n",
    "    \n",
    "            # Get the object detection graph\n",
    "            graph = self.graphs(img)\n",
    "\n",
    "            # Cache the processed data\n",
    "            with open(cache_path, 'wb') as f:\n",
    "                pickle.dump((graph), f)\n",
    "\n",
    "        return img, caption, graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestCapsCollate:\n",
    "    def __init__(self, pad_idx, batch_first=False):\n",
    "        self.pad_idx = pad_idx\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        # Extract caption_vecs, graphs, and encode_imgs from the batch\n",
    "        img = [item[0] for item in batch]\n",
    "        caption = [item[1] for item in batch]\n",
    "        graphs = [item[2] for item in batch]\n",
    "\n",
    "        img = torch.stack(img, dim=0)\n",
    "        \n",
    "        # Batch the graphs to handle multiple graphs in a single input\n",
    "        batched_graphs = dgl.batch(graphs)\n",
    "\n",
    "        return img, caption, batched_graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9KLS7DYIs4kd",
    "outputId": "35ef45db-5845-4910-f8b3-588fe25d2ceb"
   },
   "outputs": [],
   "source": [
    "#Initiate the Dataset and Dataloader\n",
    "\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "#defining the transform to be applied\n",
    "transforms = T.Compose([\n",
    "    T.Resize(226),\n",
    "    T.RandomCrop(224),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.485, 0.456, 0.406),(0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "\n",
    "#dataset class\n",
    "train_dataset =  FlickrDataset(\n",
    "    root_dir = data_location,\n",
    "    captions_file = \"D:\\\\WORK\\\\M.SC\\\\MSC_Project\\\\GitHub\\\\Image2Description\\\\train_captions.txt\",\n",
    "    cache_dir='C:\\\\Users\\\\EXTREME_RED\\\\Downloads\\\\Train_Cache\\\\',\n",
    "    transform=transforms\n",
    ")\n",
    "\n",
    "test_dataset =  test_FlickrDataset(\n",
    "    root_dir = data_location,\n",
    "    captions_file = \"D:\\\\WORK\\\\M.SC\\\\MSC_Project\\\\GitHub\\\\Image2Description\\\\test_captions.txt\",\n",
    "    cache_dir='C:\\\\Users\\\\EXTREME_RED\\\\Downloads\\\\Test_Cache\\\\',\n",
    "    transform=transforms\n",
    ")\n",
    "\n",
    "#token to represent the padding\n",
    "pad_idx = train_dataset.vocab.stoi[\"<PAD>\"]\n",
    "\n",
    "data_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=CapsCollate(pad_idx=pad_idx,batch_first=True)\n",
    "    # batch_first=False\n",
    ")\n",
    "\n",
    "test_data_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    collate_fn=TestCapsCollate(pad_idx=pad_idx,batch_first=True)\n",
    "    # batch_first=False\n",
    ")\n",
    "#vocab_size\n",
    "vocab_size = len(train_dataset.vocab)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "dB1Zq7x-s4kf",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "\n",
    "        # Load pre-trained VGG19 for feature extraction\n",
    "        vgg = models.vgg19(pretrained=pretrained)\n",
    "        for param in vgg.parameters():\n",
    "            param.requires_grad_(False)\n",
    "        self.vgg_features = vgg.features[:36]\n",
    "        self.vgg_features = nn.Sequential(*self.vgg_features)\n",
    "\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.vgg_features(images)  # (batch_size, 512, 14, 14)\n",
    "\n",
    "        # Reshape the feature map\n",
    "        features = features.view(features.size(0), -1, 49)  # (batch_size, 2048, 49)\n",
    "        features = features.permute(0, 2, 1)  # (batch_size, 49, 2048)\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "PYVM3FC4s4kg",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        # Define GCN layers\n",
    "        self.conv1 = GraphConv(in_feats, in_feats,allow_zero_in_degree=True)\n",
    "        self.conv2 = GraphConv(in_feats, in_feats,allow_zero_in_degree=True)\n",
    "\n",
    "        # Define linear layer for edge computation\n",
    "        self.edge_linear = nn.Linear(in_feats * 2, in_feats)\n",
    "\n",
    "    def forward(self, g):\n",
    "\n",
    "        h = g.ndata['features']\n",
    "        # Perform GCN\n",
    "        h = self.conv1(g, h)\n",
    "        h = torch.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        h = torch.relu(h)\n",
    "        # Concatenate src and dest node features for all edges\n",
    "        src_feats = h[g.edges()[0]]\n",
    "        dst_feats = h[g.edges()[1]]\n",
    "\n",
    "        edge_feats = torch.cat([src_feats, dst_feats], dim=2)\n",
    "        edge_values = self.edge_linear(edge_feats).squeeze(dim=1)\n",
    "\n",
    "        g.edata['edge_values'] = edge_values\n",
    "\n",
    "        mean_edge_feat = dgl.readout_edges(g, 'edge_values', op='mean')\n",
    "\n",
    "        return mean_edge_feat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Gr-AYDWCs4ki",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads, attention_dim):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_dim = attention_dim\n",
    "        self.head_dim = attention_dim // num_heads\n",
    "\n",
    "        self.linear = nn.Linear(input_dim, attention_dim)\n",
    "        self.output_linear = nn.Linear(attention_dim, input_dim)\n",
    "\n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        # Use Xavier/Glorot initialization for the linear layers\n",
    "        init.xavier_uniform_(self.linear.weight)\n",
    "        init.xavier_uniform_(self.output_linear.weight)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        batch_size, seq_length, input_dim = inputs.size()\n",
    "\n",
    "        projected_inputs = self.linear(inputs)  # (batch_size, seq_length, attention_dim)\n",
    "        reshaped_inputs = projected_inputs.view(batch_size, seq_length, self.num_heads, self.head_dim) \\\n",
    "                                          .permute(0, 2, 1, 3)  # (batch_size, num_heads, seq_length, head_dim)\n",
    "\n",
    "        # Calculate attention energy\n",
    "        transposed_inputs = reshaped_inputs.transpose(-2, -1)  # (batch_size, num_heads, head_dim, seq_length)\n",
    "        energy = torch.matmul(reshaped_inputs, transposed_inputs)  # (batch_size, num_heads, seq_length, seq_length)\n",
    "\n",
    "        # Calculate attention weights\n",
    "        attention = F.softmax(energy / torch.sqrt(torch.tensor(self.head_dim, dtype=inputs.dtype)), dim=-1)  # (batch_size, num_heads, seq_length, seq_length)\n",
    "\n",
    "        # Apply attention to get the context\n",
    "        context = torch.matmul(attention, reshaped_inputs)  # (batch_size, num_heads, seq_length, head_dim)\n",
    "\n",
    "        # Rearrange context and prepare output\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_length, self.attention_dim)  # (batch_size, seq_length, attention_dim)\n",
    "        output = self.output_linear(context)  # (batch_size, seq_length, input_dim)\n",
    "        attention = attention.permute(0, 2, 1, 3)  # (batch_size, seq_length, num_heads , seq_length)\n",
    "\n",
    "        # Optionally return attention for visualization/debugging\n",
    "        return output, attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "uDOga-6Ss4kj",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class MultiHeadCrossAttention(nn.Module):\n",
    "    def __init__(self, encoder_dim, decoder_dim, num_heads, attention_dim):\n",
    "        super(MultiHeadCrossAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_dim = attention_dim\n",
    "        self.head_dim = attention_dim // num_heads\n",
    "\n",
    "        self.encoder_linear = nn.Linear(encoder_dim, attention_dim)\n",
    "        self.decoder_linear = nn.Linear(decoder_dim, self.head_dim)\n",
    "        self.output_linear = nn.Linear(attention_dim, decoder_dim)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        # Initialize weights with Xavier/Glorot\n",
    "        init.xavier_uniform_(self.encoder_linear.weight)\n",
    "        init.xavier_uniform_(self.decoder_linear.weight)\n",
    "        init.xavier_uniform_(self.output_linear.weight)\n",
    "\n",
    "    def forward(self, encoder_out, decoder_hidden):\n",
    "        batch_size, num_features, _ = encoder_out.size()\n",
    "\n",
    "        encoder_proj = self.encoder_linear(encoder_out)  # (batch_size, num_features, attention_dim)\n",
    "        decoder_proj = self.decoder_linear(decoder_hidden)  # (batch_size, head_dim)\n",
    "\n",
    "        # Prepare the reshaped encoder projection\n",
    "        encoder_proj = encoder_proj.view(batch_size, self.num_heads, num_features, self.head_dim)  # (batch_size, num_heads, num_features, head_dim)\n",
    "\n",
    "        # Reshape decoder projection for cross-attention\n",
    "        decoder_proj = decoder_proj.unsqueeze(1).unsqueeze(1)  # (batch_size, 1, 1, head_dim)\n",
    "\n",
    "        # Compute energy for cross-attention\n",
    "        energy = torch.matmul(encoder_proj, decoder_proj.transpose(-2, -1))  # (batch_size, num_heads, num_features, 1)\n",
    "\n",
    "        # Apply scaled dot-product softmax for attention scores\n",
    "        attention = F.softmax(energy / torch.sqrt(torch.tensor(self.head_dim, dtype=encoder_out.dtype)), dim=-2)  # (batch_size, num_heads, num_features, 1)\n",
    "\n",
    "        # Get context by applying attention to encoder projections\n",
    "        context = torch.matmul(attention.transpose(-2, -1), encoder_proj).squeeze(-2)  # (batch_size, num_heads, head_dim)\n",
    "\n",
    "        # Merge multi-head context into one attention_dim\n",
    "        context = context.view(batch_size, self.head_dim * self.num_heads)  # (batch_size, attention_dim)\n",
    "\n",
    "        # Final output with a linear layer\n",
    "        output = self.output_linear(context)  # (batch_size, decoder_dim)\n",
    "\n",
    "        return output, attention.squeeze(-1)  # Squeeze to get (batch_size, num_heads, num_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "c33MuTUWs4kk"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embed_size, vocab_size, attention_dim, gcn_encode_dim, encoder_dim, decoder_dim, num_heads, drop_prob=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        # Save the model parameters\n",
    "        self.vocab_size = vocab_size\n",
    "        self.attention_dim = attention_dim\n",
    "        self.decoder_dim = decoder_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.edge_feature = GCN(gcn_encode_dim)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        # Multi-head cross attention\n",
    "        self.attention = MultiHeadCrossAttention(encoder_dim, decoder_dim, num_heads, attention_dim)\n",
    "        # Multi-head self-attention\n",
    "        self.attention_self = MultiHeadSelfAttention(embed_size, num_heads, attention_dim)\n",
    "\n",
    "        self.init_h = nn.Linear(encoder_dim, decoder_dim)\n",
    "        self.init_c = nn.Linear(encoder_dim, decoder_dim)\n",
    "        self.lstm_cell = nn.LSTMCell(embed_size + decoder_dim, decoder_dim, bias=True)\n",
    "        self.encoded = nn.Dropout(drop_prob)\n",
    "        self.fcn = nn.Linear(decoder_dim, vocab_size)\n",
    "        self.drop = nn.Dropout(drop_prob)\n",
    "\n",
    "    def forward(self, encoded_input, captions, graphs):\n",
    "        embeds = self.embedding(captions)\n",
    "\n",
    "        seq_length = len(captions[0])-1 # Exclude the last one\n",
    "        batch_size = captions.size(0)\n",
    "\n",
    "        edge_features = self.edge_feature(graphs)\n",
    "        #encoder_out = encoded_input + edge_features\n",
    "        encoder_out = torch.cat((encoded_input, edge_features), dim=2)\n",
    "        encoder_out = self.encoded(encoder_out)\n",
    "\n",
    "        h, c = self.init_hidden_state(encoder_out)\n",
    "\n",
    "        preds = torch.zeros(batch_size, seq_length, self.vocab_size).to(encoder_out.device)\n",
    "        alphas = torch.zeros(batch_size, seq_length, self.num_heads,  encoder_out.size(1)).to(encoder_out.device)\n",
    "        alphas_self = torch.zeros(batch_size,  seq_length+1, self.num_heads,  (seq_length+1)).to(encoder_out.device)\n",
    "\n",
    "        for s in range(seq_length):\n",
    "            # Multi-head self-attention over captions\n",
    "            embeds_attended_self, alpha_self = self.attention_self(embeds)\n",
    "\n",
    "            # Multi-head cross-attention with encoder output\n",
    "            output, alpha = self.attention(encoder_out, h)\n",
    "\n",
    "            lstm_input = torch.cat((embeds_attended_self[:, s], output), dim=1)\n",
    "            h, c = self.lstm_cell(lstm_input, (h, c))\n",
    "            output = self.fcn(self.drop(h))\n",
    "\n",
    "            preds[:, s] = output\n",
    "            alphas[:, s] = alpha\n",
    "            alphas_self[:,s] = alpha_self[:,-1,:,:]\n",
    "\n",
    "        return preds, alphas, alphas_self\n",
    "\n",
    "    def generate_caption(self, encoder_out, max_len=25, vocab=None):\n",
    "        batch_size = encoder_out.size(0)\n",
    "        h, c = self.init_hidden_state(encoder_out)\n",
    "\n",
    "        alphas = []\n",
    "        alphas_self = []\n",
    "        captions = []\n",
    "\n",
    "        word = torch.tensor(vocab.stoi['<SOS>']).view(1, -1).to(encoder_out.device)\n",
    "        embeds = self.embedding(word)\n",
    "\n",
    "        for i in range(max_len):\n",
    "            embeds_attended_self, alpha_self = self.attention_self(embeds)\n",
    "            alphas_self.append(alpha_self.cpu().detach().numpy())\n",
    "\n",
    "            output, alpha = self.attention(encoder_out, h)\n",
    "            alphas.append(alpha.cpu().detach().numpy())\n",
    "\n",
    "            lstm_input = torch.cat((embeds_attended_self[:, 0], output), dim=1)\n",
    "            h, c = self.lstm_cell(lstm_input, (h, c))\n",
    "            output = self.fcn(h)\n",
    "            output = output.view(batch_size, -1)\n",
    "\n",
    "            predicted_word_idx = output.argmax(dim=1)\n",
    "            if vocab.itos[predicted_word_idx.item()] == \"<EOS>\":\n",
    "                break\n",
    "                \n",
    "            captions.append(predicted_word_idx.item())\n",
    "            embeds = self.embedding(predicted_word_idx.unsqueeze(0))\n",
    "\n",
    "        return [vocab.itos[idx] for idx in captions], alphas, alphas_self\n",
    "        \n",
    "\n",
    "    def init_hidden_state(self, encoder_out):\n",
    "        mean_encoder_out = encoder_out.mean(dim=1)\n",
    "        h = self.init_h(mean_encoder_out)\n",
    "        c = self.init_c(mean_encoder_out)\n",
    "        return h, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "aQVN53IPs4kl",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self,embed_size, vocab_size, attention_dim,gcn_encode_dim,encoder_dim,decoder_dim,drop_prob=0.3):\n",
    "        super().__init__()\n",
    "        self.encoder = EncoderCNN()\n",
    "        #self.obj_feature = ObjectDetectorAndFeatureExtractor()\n",
    "        #self.edge_feature = GCN(2048, 512, 2048)\n",
    "        self.decoder = Decoder(\n",
    "            embed_size=embed_size,\n",
    "            vocab_size = len(train_dataset.vocab),\n",
    "            attention_dim=attention_dim,\n",
    "            gcn_encode_dim = gcn_encode_dim,\n",
    "            encoder_dim=encoder_dim,\n",
    "            decoder_dim=decoder_dim,\n",
    "            num_heads = 2\n",
    "        )\n",
    "\n",
    "    def forward(self, images, captions, graphs):\n",
    "        features = self.encoder(images)\n",
    "\n",
    "        outputs = self.decoder(features, captions, graphs)\n",
    "\n",
    "        #print(graphs[0])\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "__bL-MA4s4km",
    "outputId": "ad7ec9f1-f93c-45f9-8563-ecf01aac4b17",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#init model\n",
    "model = EncoderDecoder(\n",
    "    embed_size=200,\n",
    "    vocab_size = len(train_dataset.vocab),\n",
    "    attention_dim=256,\n",
    "    gcn_encode_dim = 2048,\n",
    "    encoder_dim=4096,\n",
    "    decoder_dim=256\n",
    ").to(device)\n",
    "\n",
    "print(model)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"Total parameters:\", total_params)\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.shape)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=train_dataset.vocab.stoi[\"<PAD>\"])\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O-9XXHyos4kn",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "print_every = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "for epoch in range(1,num_epochs+1):\n",
    "    for idx, (image, captions, graph) in enumerate(iter(data_loader)):\n",
    "        image,captions, graph= image.to(device),captions.to(device), graph.to(device)\n",
    "\n",
    "        # Zero the gradients.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Feed forward\n",
    "        outputs,attentions_cross, attentions_self = model(image, captions, graph)\n",
    "\n",
    "        # Calculate the batch loss.\n",
    "        targets = captions[:,1:]\n",
    "        loss = criterion(outputs.view(-1, vocab_size), targets.reshape(-1))\n",
    "\n",
    "        # Backward pass.\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the parameters in the optimizer.\n",
    "        optimizer.step()\n",
    "\n",
    "        if (idx+1)%print_every == 0:\n",
    "            print(\"Epoch: {} loss: {:.5f}\".format(epoch,loss.item()))\n",
    "\n",
    "            #generate the caption\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                dataiter = iter(test_data_loader)\n",
    "                img,_,graph = next(dataiter)\n",
    "                features = model.encoder(img[0:1].to(device))\n",
    "                edge_features = model.decoder.edge_feature(graph.to(device))\n",
    "                encoded_input= torch.cat((features, edge_features), dim=2)\n",
    "                caps,alphas,alphas_self = model.decoder.generate_caption(encoded_input,vocab=train_dataset.vocab)\n",
    "                caption = ' '.join(caps)\n",
    "                show_image(img[0],title=caption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SbeQZltLs4ko"
   },
   "outputs": [],
   "source": [
    "#torch.save(model, 'D:\\\\WORK\\\\M.SC\\\\MSC_Project\\\\GitHub\\\\Image2Description\\\\model_gcn_attention.pth')## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uK-9vr0ks4kp",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#helper function to save the model\n",
    "def save_model(model,num_epochs):\n",
    "    model_state = {\n",
    "        'num_epochs':num_epochs,\n",
    "        'embed_size':200,\n",
    "        'vocab_size':len(train_dataset.vocab),\n",
    "        'attention_dim':256,\n",
    "        'encoder_dim':2048,\n",
    "        'gcn_encoder_dim':4096,\n",
    "        'decoder_dim':256,\n",
    "        'state_dict':model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }\n",
    "\n",
    "    torch.save(model_state,'D:\\\\WORK\\\\M.SC\\\\MSC_Project\\\\GitHub\\\\Image2Description\\\\model_gcn_attention.pth')\n",
    "\n",
    "#save the latest model\n",
    "save_model(model,epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kXRKC-5cs4kr",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Load the checkpoint\n",
    "checkpoint = torch.load('D:\\\\WORK\\\\M.SC\\\\MSC_Project\\\\GitHub\\\\Image2Description\\\\model_gcn_attention.pth',map_location=device)\n",
    "learning_rate = 0.0001\n",
    "\n",
    "# Reinitialize the model and optimizer\n",
    "model = EncoderDecoder(\n",
    "    embed_size=200,\n",
    "    vocab_size = len(train_dataset.vocab),\n",
    "    attention_dim=256,\n",
    "    gcn_encode_dim = 2048,\n",
    "    encoder_dim=4096,\n",
    "    decoder_dim=256\n",
    ").to(device) \n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "# Load other saved data\n",
    "start_epoch = checkpoint['num_epochs'] + 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w3wJOec6s4ks",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print_every = 10\n",
    "\n",
    "for epoch in range(start_epoch, 16):\n",
    "    for idx, (image, captions, graph) in enumerate(iter(data_loader)):\n",
    "        image,captions, graph= image.to(device),captions.to(device), graph.to(device)\n",
    "\n",
    "        # Zero the gradients.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Feed forward\n",
    "        outputs,attentions_cross, attentions_self = model(image, captions, graph)\n",
    "\n",
    "        # Calculate the batch loss.\n",
    "        targets = captions[:,1:]\n",
    "        loss = criterion(outputs.view(-1, vocab_size), targets.reshape(-1))\n",
    "\n",
    "        # Backward pass.\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the parameters in the optimizer.\n",
    "        optimizer.step()\n",
    "\n",
    "        if (idx+1)%print_every == 0:\n",
    "            print(\"Epoch: {} loss: {:.5f}\".format(epoch,loss.item()))\n",
    "\n",
    "            #generate the caption\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                dataiter = iter(test_data_loader)\n",
    "                img,_,graph = next(dataiter)\n",
    "                features = model.encoder(img[0:1].to(device))\n",
    "                edge_features = model.decoder.edge_feature(graph.to(device))\n",
    "                encoded_input= torch.cat((features, edge_features), dim=2)\n",
    "                caps,alphas,alphas_self = model.decoder.generate_caption(encoded_input,vocab=train_dataset.vocab)\n",
    "                caption = ' '.join(caps)\n",
    "                show_image(img[0],title=caption)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hguxs1RUs4kt"
   },
   "outputs": [],
   "source": [
    "#helper function to save the model\n",
    "def save_model(model,num_epochs):\n",
    "    model_state = {\n",
    "        'num_epochs':num_epochs,\n",
    "        'embed_size':200,\n",
    "        'vocab_size':len(train_dataset.vocab),\n",
    "        'attention_dim':256,\n",
    "        'encoder_dim':2048,\n",
    "        'gcn_encoder_dim':4096,\n",
    "        'decoder_dim':256,\n",
    "        'state_dict':model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }\n",
    "\n",
    "    torch.save(model_state,'D:\\\\WORK\\\\M.SC\\\\MSC_Project\\\\GitHub\\\\Image2Description\\\\model_gcn_attention_Final.pth')\n",
    "\n",
    "#save the latest model\n",
    "save_model(model,epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "amIik8VNs4ku"
   },
   "source": [
    "## Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "rFS9fgWGs4kx"
   },
   "outputs": [],
   "source": [
    "#generate caption\n",
    "def get_caps_from(features_tensors,graph):\n",
    "    #generate the caption\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        features = model.encoder(features_tensors.to(device))\n",
    "        edge_features = model.decoder.edge_feature(graph.to(device))\n",
    "        encoded_input= torch.cat((features, edge_features), dim=2)\n",
    "        caps,alphas,alphas_self = model.decoder.generate_caption(encoded_input,vocab=train_dataset.vocab)\n",
    "        caption = ' '.join(caps)\n",
    "        show_image(features_tensors[0],title=caption)\n",
    "\n",
    "    return caps,alphas, alphas_self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pJa4lP8ts4kz"
   },
   "outputs": [],
   "source": [
    "#show any 1\n",
    "dataiter = iter(test_data_loader)\n",
    "images,caption,graph = next(dataiter)\n",
    "\n",
    "img = images[0].detach().clone()\n",
    "img1 = images[0].detach().clone()\n",
    "caps,alphas,alphas_self = get_caps_from(img.unsqueeze(0),graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show all the test samples\n",
    "for idx, (image, captions, graph) in enumerate(iter(test_data_loader)):\n",
    "  img = image[0].detach().clone()\n",
    "  img1 = image[0].detach().clone()\n",
    "  caps,alphas,alphas_self = get_caps_from(img.unsqueeze(0),graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculated ROUGE & CIDEr \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import defaultdict, Counter\n",
    "import math\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# CIDEr Functions\n",
    "def compute_ngrams(sentence, n=1):\n",
    "    \"\"\"Generate n-grams from a sentence.\"\"\"\n",
    "    tokens = sentence.lower().split()\n",
    "    ngrams = zip(*[tokens[i:] for i in range(n)])\n",
    "    return Counter([' '.join(ngram) for ngram in ngrams])\n",
    "\n",
    "def compute_term_frequency(ngrams):\n",
    "    \"\"\"Calculate term frequency (TF) for n-grams.\"\"\"\n",
    "    total_ngrams = sum(ngrams.values())\n",
    "    return {ngram: count / total_ngrams for ngram, count in ngrams.items()}\n",
    "\n",
    "def compute_inverse_document_frequency(reference_captions, n=1):\n",
    "    \"\"\"Calculate inverse document frequency (IDF) for each n-gram across all references.\"\"\"\n",
    "    doc_freq = defaultdict(int)\n",
    "    num_documents = len(reference_captions)\n",
    "    \n",
    "    for ref in reference_captions:\n",
    "        ngrams = compute_ngrams(ref, n)\n",
    "        for ngram in ngrams:\n",
    "            doc_freq[ngram] += 1\n",
    "    \n",
    "    idf = {ngram: math.log((num_documents / (count + 1)) + 1) for ngram, count in doc_freq.items()}\n",
    "    return idf\n",
    "\n",
    "def compute_cosine_similarity(tf_ref, tf_gen, idf):\n",
    "    \"\"\"Compute cosine similarity between two term frequency vectors (TF-IDF weighted).\"\"\"\n",
    "    # Get all unique n-grams\n",
    "    ngrams = set(tf_ref.keys()).union(set(tf_gen.keys()))\n",
    "    \n",
    "    # Compute TF-IDF for both reference and generated captions\n",
    "    tfidf_ref = {ngram: tf_ref.get(ngram, 0) * idf.get(ngram, 0) for ngram in ngrams}\n",
    "    tfidf_gen = {ngram: tf_gen.get(ngram, 0) * idf.get(ngram, 0) for ngram in ngrams}\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    dot_product = sum(tfidf_ref[ngram] * tfidf_gen[ngram] for ngram in ngrams)\n",
    "    norm_ref = math.sqrt(sum(val ** 2 for val in tfidf_ref.values()))\n",
    "    norm_gen = math.sqrt(sum(val ** 2 for val in tfidf_gen.values()))\n",
    "    \n",
    "    if norm_ref == 0 or norm_gen == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return dot_product / (norm_ref * norm_gen)\n",
    "\n",
    "def compute_cider(generated_caption, reference_captions, n=1):\n",
    "    \"\"\"Compute the CIDEr score for one generated caption and a set of reference captions.\"\"\"\n",
    "    # Compute n-grams for generated caption\n",
    "    ngrams_gen = compute_ngrams(generated_caption, n)\n",
    "    \n",
    "    # Compute term frequencies (TF) for generated caption\n",
    "    tf_gen = compute_term_frequency(ngrams_gen)\n",
    "    \n",
    "    # Compute IDF using all reference captions\n",
    "    idf = compute_inverse_document_frequency(reference_captions, n)\n",
    "    \n",
    "    cider_score = 0.0\n",
    "    for ref_caption in reference_captions:\n",
    "        # Compute n-grams for reference caption\n",
    "        ngrams_ref = compute_ngrams(ref_caption, n)\n",
    "        \n",
    "        # Compute term frequencies (TF) for reference caption\n",
    "        tf_ref = compute_term_frequency(ngrams_ref)\n",
    "        \n",
    "        # Compute cosine similarity between the generated and reference caption\n",
    "        similarity = compute_cosine_similarity(tf_ref, tf_gen, idf)\n",
    "        cider_score += similarity\n",
    "    \n",
    "    # Average similarity across all reference captions\n",
    "    cider_score /= len(reference_captions)\n",
    "    \n",
    "    return cider_score\n",
    "\n",
    "# Model Caption Generation Function\n",
    "def get_caps_from(features_tensors, graph):\n",
    "    \"\"\"Generate the caption using the model.\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        features = model.encoder(features_tensors.to(device))\n",
    "        edge_features = model.decoder.edge_feature(graph.to(device))\n",
    "        encoded_input = torch.cat((features, edge_features), dim=2)\n",
    "        caps, alphas, alphas_self = model.decoder.generate_caption(encoded_input, vocab=train_dataset.vocab)\n",
    "        \n",
    "        # Join the words and ensure capitalization and punctuation\n",
    "        caption = ' '.join(caps)\n",
    "        caption = caption.capitalize()  # Capitalize the first word\n",
    "        if not caption.endswith('.'):\n",
    "            caption += '.'  # Ensure the caption ends with a period\n",
    "        #show_image(features_tensors[0],title=caption)    \n",
    "        return caps, caption\n",
    "\n",
    "# Initialize ROUGE scorer\n",
    "rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2'], use_stemmer=True)\n",
    "\n",
    "# Lists to hold generated captions and reference captions\n",
    "generated_captions = []\n",
    "reference_captions = []\n",
    "\n",
    "# Iterate through the test data loader\n",
    "for i, (images, references, graph) in enumerate(test_data_loader):\n",
    "    img = images[0].detach().clone()\n",
    "    \n",
    "    # Generate the caption\n",
    "    caps, generated_caption = get_caps_from(img.unsqueeze(0), graph)\n",
    "    error = '<pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>.' #This is due to some bug in model code where randomly for any input the model will output all <PAD>, i.e 0.\n",
    "    # Add to the generated captions list\n",
    "    if generated_caption == error:\n",
    "        continue    \n",
    "    '''print(\"<==========>\",i)\n",
    "    print(caps)\n",
    "    print(generated_caption)\n",
    "    print(references)'''\n",
    "    # Store generated and reference captions\n",
    "    generated_captions.append(generated_caption)\n",
    "    reference_captions.append(references[0])  # Assuming a single reference per image\n",
    "    \n",
    "\n",
    "# Initialize ROUGE scores\n",
    "rouge1_scores = []\n",
    "rouge2_scores = []\n",
    "\n",
    "# Initialize CIDEr scores\n",
    "cider_unigram_scores = []\n",
    "cider_bigram_scores = []\n",
    "\n",
    "for gen_cap, ref_cap in zip(generated_captions, reference_captions):\n",
    "    # Compute ROUGE-1 and ROUGE-2\n",
    "    scores = rouge_scorer.score(ref_cap, gen_cap)\n",
    "    rouge1_scores.append(scores['rouge1'].fmeasure)\n",
    "    rouge2_scores.append(scores['rouge2'].fmeasure)\n",
    "    \n",
    "    # Compute CIDEr scores\n",
    "    cider_unigram = compute_cider(gen_cap, [ref_cap], n=1)\n",
    "    cider_bigram = compute_cider(gen_cap, [ref_cap], n=2)\n",
    "    cider_unigram_scores.append(cider_unigram)\n",
    "    cider_bigram_scores.append(cider_bigram)\n",
    "\n",
    "# Average ROUGE and CIDEr scores\n",
    "avg_rouge1 = sum(rouge1_scores) / len(rouge1_scores)\n",
    "avg_rouge2 = sum(rouge2_scores) / len(rouge2_scores)\n",
    "avg_cider_unigram = sum(cider_unigram_scores) / len(cider_unigram_scores)\n",
    "avg_cider_bigram = sum(cider_bigram_scores) / len(cider_bigram_scores)\n",
    "\n",
    "# Print results\n",
    "print(f\"Average ROUGE-1 Score: {avg_rouge1}\")\n",
    "print(f\"Average ROUGE-2 Score: {avg_rouge2}\")\n",
    "print(f\"Average CIDEr Score (Unigram): {avg_cider_unigram}\")\n",
    "print(f\"Average CIDEr Score (Bigram): {avg_cider_bigram}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
